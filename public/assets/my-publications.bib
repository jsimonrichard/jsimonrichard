@article{40YearsQuantum2022,
  title = {40 Years of Quantum Computing},
  year = {2022},
  month = jan,
  journal = {Nature Reviews Physics},
  volume = {4},
  number = {1},
  pages = {1--1},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00410-6},
  abstract = {This year we celebrate four decades of quantum computing by looking back at the milestones of the field and forward to the challenges and opportunities that lie ahead.}
}

@article{aaronsonReadFinePrint2015,
  title = {Read the Fine Print},
  author = {Aaronson, Scott},
  year = {2015},
  month = apr,
  journal = {Nature Physics},
  volume = {11},
  number = {4},
  pages = {291--293},
  publisher = {Nature Publishing Group},
  issn = {1745-2481},
  doi = {10.1038/nphys3272},
  urldate = {2024-05-07},
  abstract = {New quantum algorithms promise an exponential speed-up for machine learning, clustering and finding patterns in big data. But to achieve a real speed-up, we need to delve into the details.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Quantum information},
  file = {/home/jsimonrichard/Zotero/storage/IA5Q3JNI/Aaronson - 2015 - Read the fine print.pdf}
}

@misc{adhikarySupervisedLearningQuantum2019,
  title = {Supervised Learning with a Quantum Classifier Using a Multi-Level System},
  author = {Adhikary, Soumik and Dangwal, Siddharth and Bhowmik, Debanjan},
  year = {2019},
  month = aug,
  journal = {arXiv.org},
  doi = {10.1007/s11128-020-2587-9},
  urldate = {2023-09-17},
  abstract = {We propose a quantum classifier, which can classify data under the supervised learning scheme using a quantum feature space. The input feature vectors are encoded in a single qu\$N\$it (a \$N\$ level quantum system), as opposed to more commonly used entangled multi-qubit systems. For training we use the much used quantum variational algorithm -- a hybrid quantum-classical algorithm -- in which the forward part of the computation is performed on a quantum hardware whereas the feedback part is carried out on a classical computer. We introduce "single shot training" in our scheme, with all input samples belonging to the same class being used to train the classifier simultaneously. This significantly speeds up the training procedure and provides an advantage over classical machine learning classifiers. We demonstrate successful classification of popular benchmark datasets with our quantum classifier and compare its performance with respect to some classical machine learning classifiers. We also show that the number of training parameters in our classifier is significantly less than the classical classifiers.},
  howpublished = {https://arxiv.org/abs/1908.08385v1},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/U56ENMYP/Adhikary et al. - 2019 - Supervised learning with a quantum classifier usin.pdf}
}

@misc{AIAchievesSilvermedal2024,
  title = {{{AI}} Achieves Silver-Medal Standard Solving {{International Mathematical Olympiad}} Problems},
  year = {2024},
  month = aug,
  journal = {Google DeepMind},
  urldate = {2024-08-03},
  abstract = {Breakthrough models AlphaProof and AlphaGeometry 2 solve advanced reasoning problems in mathematics},
  howpublished = {https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/SS3VDIZZ/ai-solves-imo-problems-at-silver-medal-level.html}
}

@misc{aiQuantumGraphNeural2023,
  title = {Towards {{Quantum Graph Neural Networks}}: {{An Ego-Graph Learning Approach}}},
  author = {Ai, Xing and Zhang, Zhihong and Sun, Luzhe and Yan, Junchi and Hancock, Edwin},
  year = {2023},
  month = mar,
  number = {arXiv:2201.05158},
  eprint = {2201.05158},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.05158},
  urldate = {2023-12-22},
  abstract = {Quantum machine learning is a fast-emerging field that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as the Ego-graph based Quantum Graph Neural Network (egoQGNN). egoQGNN implements the GNN theoretical framework using the tensor product and unity matrix representation, which greatly reduces the number of model parameters required. When controlled by a classical computer, egoQGNN can accommodate arbitrarily sized graphs by processing ego-graphs from the input graph using a modestly-sized quantum device. The architecture is based on a novel mapping from real-world data to Hilbert space. This mapping maintains the distance relations present in the data and reduces information loss. Experimental results show that the proposed method outperforms competitive state-of-the-art models with only 1.68{\textbackslash}\% parameters compared to those models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,PROTEINS Dataset,Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/HZUVU3HS/Ai et al. - 2023 - Decompositional Quantum Graph Neural Network.pdf;/home/jsimonrichard/Zotero/storage/H2B8BWSR/2201.html}
}

@inproceedings{akibaOptunaNextgenerationHyperparameter2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3292500.3330701},
  urldate = {2024-05-06},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  isbn = {978-1-4503-6201-6},
  keywords = {Bayesian optimization,black-box optimization,hyperparameter optimization,machine learning system},
  file = {/home/jsimonrichard/Zotero/storage/D5ZWWEW9/Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf;/home/jsimonrichard/Zotero/storage/63HHAWLM/1907.html}
}

@article{aminQuantumBoltzmannMachine2018,
  title = {Quantum {{Boltzmann Machine}}},
  author = {Amin, Mohammad H. and Andriyash, Evgeny and Rolfe, Jason and Kulchytskyy, Bohdan and Melko, Roger},
  year = {2018},
  month = may,
  journal = {Physical Review X},
  volume = {8},
  number = {2},
  pages = {021050},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.8.021050},
  urldate = {2023-09-30},
  abstract = {Inspired by the success of Boltzmann Machines based on classical Boltzmann distribution, we propose a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian. Due to the non-commutative nature of quantum mechanics, the training process of the Quantum Boltzmann Machine (QBM) can become nontrivial. We circumvent the problem by introducing bounds on the quantum probabilities. This allows us to train the QBM efficiently by sampling. We show examples of QBM training with and without the bound, using exact diagonalization, and compare the results with classical Boltzmann training. We also discuss the possibility of using quantum annealing processors like D-Wave for QBM training and application.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/XN5GHZLS/Amin et al. - 2018 - Quantum Boltzmann Machine.pdf}
}

@misc{AnnualThreatAssessment,
  title = {Annual {{Threat Assessment}} of the {{U}}.{{S}}. {{Intelligence Community}}},
  publisher = {Office of the Director of National Intelligence},
  urldate = {2022-11-23},
  howpublished = {https://www.intelligence.gov/annual-threat-assessment}
}

@misc{ArtUnixProgramming,
  title = {The {{Art}} of {{Unix Programming}}},
  urldate = {2023-10-10},
  howpublished = {http://www.catb.org/{\textasciitilde}esr/writings/taoup/html/},
  file = {/home/jsimonrichard/Zotero/storage/RQ28K85Y/html.html}
}

@article{averkovConvexHullsMonomial2024,
  title = {Convex Hulls of Monomial Curves, and a Sparse Positivstellensatz},
  author = {Averkov, Gennadiy and Scheiderer, Claus},
  year = {2024},
  month = feb,
  journal = {Mathematical Programming},
  issn = {1436-4646},
  doi = {10.1007/s10107-024-02060-9},
  urldate = {2024-08-25},
  abstract = {Consider the closed convex hull K of a monomial curve given parametrically as \$\$(t{\textasciicircum}\{m\_1\},{\textbackslash}ldots ,t{\textasciicircum}\{m\_n\})\$\$, with the parameter t varying in an interval I. We show, using constructive arguments, that K admits a lifted semidefinite description by \$\${\textbackslash}mathcal \{O\}(d)\$\$linear matrix inequalities (LMIs), each of size \$\${\textbackslash}left{\textbackslash}lfloor {\textbackslash}frac\{n\}\{2\} {\textbackslash}right{\textbackslash}rfloor +1\$\$, where \$\$d= {\textbackslash}max {\textbackslash}\{m\_1,{\textbackslash}ldots ,m\_n{\textbackslash}\}\$\$is the degree of the curve. On the dual side, we show that if a univariate polynomial p(t) of degree d with at most \$\$2k+1\$\$monomials is non-negative on \$\$\{{\textbackslash}mathbb \{R\}\}\_+\$\$, then p admits a representation \$\$p = t{\textasciicircum}0 {\textbackslash}sigma \_0 + {\textbackslash}cdots + t{\textasciicircum}\{d-k\} {\textbackslash}sigma \_\{d-k\}\$\$, where the polynomials \$\${\textbackslash}sigma \_0,{\textbackslash}ldots ,{\textbackslash}sigma \_\{d-k\}\$\$are sums of squares and \$\${\textbackslash}deg ({\textbackslash}sigma \_i) {\textbackslash}le 2k\$\$. The latter is a univariate positivstellensatz for sparse polynomials, with non-negativity of p being certified by sos polynomials whose degree only depends on the sparsity of~p. Our results fit into the general attempt of formulating polynomial optimization problems as semidefinite problems with LMIs of small size. Such small-size descriptions are much more tractable from a computational viewpoint.},
  langid = {english},
  keywords = {06E05,13J30,90C22,90C23},
  file = {/home/jsimonrichard/Zotero/storage/MBYSU83S/Averkov and Scheiderer - 2024 - Convex hulls of monomial curves, and a sparse posi.pdf}
}

@misc{averkovMixedVolumesZonoids2024,
  title = {Mixed Volumes of Zonoids and the Absolute Value of the {{Grassmannian}} ({{Extended Abstract}})},
  author = {Averkov, Gennadiy and von Dichter, Katherina and Richard, Simon and Soprunov, Ivan},
  year = {2024},
  month = apr,
  number = {arXiv:2404.02842},
  eprint = {2404.02842},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.02842},
  urldate = {2024-10-16},
  abstract = {Zonoids are Hausdorff limits of zonotopes, while zonotopes are convex polytopes defined as the Minkowski sums of finitely many segments. We present a combinatorial framework that links the study of mixed volumes of zonoids (a topic that has applications in algebraic combinatorics) with the study of the absolute value of the Grassmannian, defined as the image of the Grassmannian under the coordinate-wise absolute value map. We use polyhedral computations to derive new families of inequalities for n zonoids in dimension d, when (n,d)=(6,2) and (6,3). Unlike the classical geometric inequalities, originating from the Brunn-Minkowski and Aleksandrov-Fenchel inequalities, the inequalities we produce have the special feature of being Minkowski linear in each of the n zonoids they involve.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Mathematics - Algebraic Geometry,Mathematics - Combinatorics,Mathematics - Metric Geometry},
  file = {/home/jsimonrichard/Zotero/storage/KBR63XNQ/Averkov et al. - 2024 - Mixed volumes of zonoids and the absolute value of the Grassmannian (Extended Abstract).pdf;/home/jsimonrichard/Zotero/storage/CLMHPX54/2404.html}
}

@article{averkovPluckerTypeInequalitiesMixed2023,
  title = {Pl{\"u}cker-{{Type Inequalities}} for {{Mixed Areas}} and {{Intersection Numbers}} of {{Curve Arrangements}}},
  author = {Averkov, Gennadiy and Soprunov, Ivan},
  year = {2023},
  month = sep,
  journal = {International Mathematics Research Notices},
  volume = {2023},
  number = {18},
  pages = {16015--16050},
  issn = {1073-7928},
  doi = {10.1093/imrn/rnac216},
  urldate = {2024-09-06},
  abstract = {Any collection of \$n\$ compact convex planar sets \$K\_1,{\textbackslash}dots , K\_n\$ defines a vector of \$\{n{\textbackslash}choose 2\}\$ mixed areas \$\{{\textbackslash}operatorname \{V\}\}(K\_i,K\_j)\$ for \$1{\textbackslash}leq i\&lt;j{\textbackslash}leq n\$. We show that for \$n{\textbackslash}geq 4\$ these numbers satisfy certain Pl{\"u}cker-type inequalities. Moreover, we prove that for \$n=4\$, these inequalities completely describe the space of all mixed area vectors \$(\{{\textbackslash}operatorname \{V\}\}(K\_i,K\_j){\textbackslash},:{\textbackslash},1{\textbackslash}leq i\&lt;j{\textbackslash}leq 4)\$. For arbitrary \$n{\textbackslash}geq 4\$, we show that this space has a semialgebraic closure of full dimension. As an application, we show that the pairwise intersection numbers of any collection of \$n\$ tropical curves satisfy the Pl{\"u}cker-type inequalities. Moreover, in the case of four tropical curves, any homogeneous polynomial relation between their six intersection numbers follows from the corresponding Pl{\"u}cker-type inequalities.},
  file = {/home/jsimonrichard/Zotero/storage/SZE8V497/Averkov and Soprunov - 2023 - Plücker-Type Inequalities for Mixed Areas and Intersection Numbers of Curve Arrangements.pdf;/home/jsimonrichard/Zotero/storage/RL9DD74D/6658476.html}
}

@inproceedings{baczykNoiseInfluenceQuantum2021,
  title = {Noise Influence on {{Quantum Machine Learning}} Models' Performance},
  author = {Baczyk, Michal},
  year = {2021},
  urldate = {2023-09-30},
  abstract = {We analyse how the training and performance of VQC models is affected by noise inherent to NISQ devices. In particular, we study the influence of three different types of quantum hardware noise: measurement errors, single qubit gate errors, and two-qubit gate errors (e.g., CNOT gate). Furthermore, we train the previously mentioned QML algorithms using noise models that emulate the behaviour of available quantum computers with high accuracy. We conclude that the tested QML models are suitable for operation on current NISQ devices.},
  file = {/home/jsimonrichard/Zotero/storage/4484VJT3/Baczyk - 2021 - Noise inﬂuence on Quantum Machine Learning models’.pdf}
}

@article{bangStrategyQuantumAlgorithm2014,
  title = {A Strategy for Quantum Algorithm Design Assisted by Machine Learning},
  author = {Bang, Jeongho and Ryu, Junghee and Yoo, Seokwon and Paw{\l}owski, Marcin and Lee, Jinhyoung},
  year = {2014},
  month = jul,
  journal = {New Journal of Physics},
  volume = {16},
  number = {7},
  pages = {073017},
  publisher = {IOP Publishing},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/16/7/073017},
  urldate = {2023-09-11},
  abstract = {We propose a method for quantum algorithm design assisted by machine learning. The method uses a quantum--classical hybrid simulator, where a `quantum student' is being taught by a `classical teacher'. In other words, in our method, the learning system is supposed to evolve into a quantum algorithm for a given problem, assisted by a classical main-feedback system. Our method is applicable for designing quantum oracle-based algorithms. We chose, as a case study, an oracle decision problem, called a Deutsch--Jozsa problem. We showed by using Monte Carlo simulations that our simulator can faithfully learn a quantum algorithm for solving the problem for a given oracle. Remarkably, the learning time is proportional to the square root of the total number of parameters, rather than showing the exponential dependence found in the classical machine learning-based method.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/LZI48XHL/Bang et al. - 2014 - A strategy for quantum algorithm design assisted b.pdf}
}

@article{beerQuantumMachineLearning2023,
  title = {Quantum Machine Learning of Graph-Structured Data},
  author = {Beer, Kerstin and Khosla, Megha and K{\"o}hler, Julius and Osborne, Tobias J. and Zhao, Tianqi},
  year = {2023},
  month = jul,
  journal = {Physical Review A},
  volume = {108},
  number = {1},
  pages = {012410},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.108.012410},
  urldate = {2024-01-21},
  abstract = {Graph structures are ubiquitous throughout the natural sciences. Here we develop an approach that exploits the quantum source's graph structure to improve learning via an arbitrary quantum neural network (QNN) ansatz. In particular, we devise and optimize a self-supervised objective to capture the information-theoretic closeness of the quantum states in the training of a QNN. Numerical simulations show that our approach improves the learning efficiency and the generalization behavior of the base QNN. On a practical note, scalable quantum implementations of the learning procedure described in this paper are likely feasible on the next generation of quantum computing devices.},
  file = {/home/jsimonrichard/Zotero/storage/IN4XFAZ9/Beer et al. - 2023 - Quantum machine learning of graph-structured data.pdf;/home/jsimonrichard/Zotero/storage/2ZVP7YRW/PhysRevA.108.html}
}

@article{benedettiGenerativeModelingApproach2019,
  title = {A Generative Modeling Approach for Benchmarking and Training Shallow Quantum Circuits},
  author = {Benedetti, Marcello and {Garcia-Pintos}, Delfina and Perdomo, Oscar and {Leyton-Ortega}, Vicente and Nam, Yunseong and {Perdomo-Ortiz}, Alejandro},
  year = {2019},
  month = may,
  journal = {npj Quantum Information},
  volume = {5},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2056-6387},
  doi = {10.1038/s41534-019-0157-8},
  urldate = {2023-10-07},
  abstract = {Hybrid quantum-classical algorithms provide ways to use noisy intermediate-scale quantum computers for practical applications. Expanding the portfolio of such techniques, we propose a quantum circuit learning algorithm that can be used to assist the characterization of quantum devices and to train shallow circuits for generative tasks. The procedure leverages quantum hardware capabilities to its fullest extent by using native gates and their qubit connectivity. We demonstrate that our approach can learn an optimal preparation of the Greenberger-Horne-Zeilinger states, also known as ``cat states''. We further demonstrate that our approach can efficiently prepare approximate representations of coherent thermal states, wave functions that encode Boltzmann probabilities in their amplitudes. Finally, complementing proposals to characterize the power or usefulness of near-term quantum devices, such as IBM's quantum volume, we provide a new hardware-independent metric called the qBAS score. It is based on the performance yield in a specific sampling task on one of the canonical machine learning data sets known as Bars and Stripes. We show how entanglement is a key ingredient in encoding the patterns of this data set; an ideal benchmark for testing hardware starting at four qubits and up. We provide experimental results and evaluation of this metric to probe the trade off between several architectural circuit designs and circuit depths on an ion-trap quantum computer.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Computational science,Quantum information},
  file = {/home/jsimonrichard/Zotero/storage/5FSLFRDN/Benedetti et al. - 2019 - A generative modeling approach for benchmarking an.pdf}
}

@misc{benedettiParameterizedQuantumCircuits2019,
  title = {Parameterized Quantum Circuits as Machine Learning Models},
  author = {Benedetti, Marcello and Lloyd, Erika and Sack, Stefan and Fiorentini, Mattia},
  year = {2019},
  month = jun,
  journal = {arXiv.org},
  doi = {10.1088/2058-9565/ab4eb5},
  urldate = {2023-09-17},
  abstract = {Hybrid quantum-classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.},
  howpublished = {https://arxiv.org/abs/1906.07682v2},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/PCVDYIJQ/Benedetti et al. - 2019 - Parameterized quantum circuits as machine learning.pdf}
}

@article{benitezCollisionDetectionUsing2005,
  title = {Collision {{Detection Using Sphere-Tree Construction}}},
  author = {Benitez, Antonio and Del Carmen Ram{\'i}rez, Maria and Vallejo, Daniel},
  year = {2005},
  month = feb,
  journal = {Electronics, Communications, and Computers, International Conference on},
  volume = {2005},
  pages = {286--291},
  publisher = {IEEE Computer Society},
  doi = {10.1109/CONIEL.2005.29},
  urldate = {2022-12-16},
  abstract = {Fast and accurate collision detection between general geometric models is a fundamental problem in modeling, robotics, manufacturing and computer-simulated environments. Most of the earlier algorithm are either restricted to a class of geometric models, say convex polytopes, or are not fast enough for practical applications. We present an new algorithm for collision detection between general polygonal models. The algorithm makes use of hierarchical representations along with frame to frame coherence to rapidly detect collisions. It has been implemented as part of motion planning package. In practice, it can accurately detect the contacts between large geometries composed of thousands of polygons at interactive rates. {\copyright} 2005 IEEE.},
  isbn = {0-7695-2283-1},
  keywords = {Collision detection,Object approximation,Sphere-tree construction}
}

@article{bergerImpactProgrammingLanguages2019,
  title = {On the {{Impact}} of {{Programming Languages}} on {{Code Quality}}: {{A Reproduction Study}}},
  shorttitle = {On the {{Impact}} of {{Programming Languages}} on {{Code Quality}}},
  author = {Berger, Emery D. and Hollenbeck, Celeste and Maj, Petr and Vitek, Olga and Vitek, Jan},
  year = {2019},
  month = oct,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {41},
  number = {4},
  pages = {21:1--21:24},
  issn = {0164-0925},
  doi = {10.1145/3340571},
  urldate = {2023-10-10},
  abstract = {In a 2014 article, Ray, Posnett, Devanbu, and Filkov claimed to have uncovered a statistically significant association between 11 programming languages and software defects in 729 projects hosted on GitHub. Specifically, their work answered four research questions relating to software defects and programming languages. With data and code provided by the authors, the present article first attempts to conduct an experimental repetition of the original study. The repetition is only partially successful, due to missing code and issues with the classification of languages. The second part of this work focuses on their main claim, the association between bugs and languages, and performs a complete, independent reanalysis of the data and of the statistical modeling steps undertaken by Ray et al. in 2014. This reanalysis uncovers a number of serious flaws that reduce the number of languages with an association with defects down from 11 to only 4. Moreover, the practical effect size is exceedingly small. These results thus undermine the conclusions of the original study. Correcting the record is important, as many subsequent works have cited the 2014 article and have asserted, without evidence, a causal link between the choice of programming language for a given task and the number of software defects. Causation is not supported by the data at hand; and, in our opinion, even after fixing the methodological flaws we uncovered, too many unaccounted sources of bias remain to hope for a meaningful comparison of bug rates across languages.},
  keywords = {Programming Languages on Code Quality},
  file = {/home/jsimonrichard/Zotero/storage/692LDXMU/Berger et al. - 2019 - On the Impact of Programming Languages on Code Qua.pdf}
}

@misc{bergholmPennyLaneAutomaticDifferentiation2022,
  title = {{{PennyLane}}: {{Automatic}} Differentiation of Hybrid Quantum-Classical Computations},
  shorttitle = {{{PennyLane}}},
  author = {Bergholm, Ville and Izaac, Josh and Schuld, Maria and Gogolin, Christian and Ahmed, Shahnawaz and Ajith, Vishnu and Alam, M. Sohaib and {Alonso-Linaje}, Guillermo and AkashNarayanan, B. and Asadi, Ali and Arrazola, Juan Miguel and Azad, Utkarsh and Banning, Sam and Blank, Carsten and Bromley, Thomas R. and Cordier, Benjamin A. and Ceroni, Jack and Delgado, Alain and Di Matteo, Olivia and Dusko, Amintor and Garg, Tanya and Guala, Diego and Hayes, Anthony and Hill, Ryan and Ijaz, Aroosa and Isacsson, Theodor and Ittah, David and Jahangiri, Soran and Jain, Prateek and Jiang, Edward and Khandelwal, Ankit and Kottmann, Korbinian and Lang, Robert A. and Lee, Christina and Loke, Thomas and Lowe, Angus and McKiernan, Keri and Meyer, Johannes Jakob and {Monta{\~n}ez-Barrera}, J. A. and Moyard, Romain and Niu, Zeyue and O'Riordan, Lee James and Oud, Steven and Panigrahi, Ashish and Park, Chae-Yeun and Polatajko, Daniel and Quesada, Nicol{\'a}s and Roberts, Chase and S{\'a}, Nahum and Schoch, Isidor and Shi, Borun and Shu, Shuli and Sim, Sukin and Singh, Arshpreet and Strandberg, Ingrid and Soni, Jay and Sz{\'a}va, Antal and Thabet, Slimane and {Vargas-Hern{\'a}ndez}, Rodrigo A. and Vincent, Trevor and Vitucci, Nicola and Weber, Maurice and Wierichs, David and Wiersema, Roeland and Willmann, Moritz and Wong, Vincent and Zhang, Shaoming and Killoran, Nathan},
  year = {2022},
  month = jul,
  number = {arXiv:1811.04968},
  eprint = {1811.04968},
  primaryclass = {physics, physics:quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.04968},
  urldate = {2024-05-12},
  abstract = {PennyLane is a Python 3 software framework for differentiable programming of quantum computers. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for hardware providers including the Xanadu Cloud, Amazon Braket, and IBM Quantum, allowing PennyLane optimizations to be run on publicly accessible quantum devices. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, JAX, and Autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Machine Learning,Physics - Computational Physics,Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/N4Q74F3K/Bergholm et al. - 2022 - PennyLane Automatic differentiation of hybrid qua.pdf;/home/jsimonrichard/Zotero/storage/SCX9TPPF/1811.html}
}

@inproceedings{bernsteinGroverVsMcEliece2010,
  title = {Grover vs. {{McEliece}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Bernstein, Daniel J.},
  year = {2010},
  volume = {6061 LNCS},
  issn = {03029743},
  doi = {10.1007/978-3-642-12929-2_6},
  abstract = {This paper shows that quantum information-set-decoding attacks are asymptotically much faster than non-quantum information-set-decoding attacks. {\copyright} 2010 Springer-Verlag.},
  file = {/home/jsimonrichard/Zotero/storage/CXA3P6B5/Bernstein - 2010 - Grover vs. McEliece.pdf}
}

@inproceedings{bhattacharyaAssessingProgrammingLanguage2011,
  title = {Assessing Programming Language Impact on Development and Maintenance: A Study on c and C++},
  shorttitle = {Assessing Programming Language Impact on Development and Maintenance},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Software Engineering}}},
  author = {Bhattacharya, Pamela and Neamtiu, Iulian},
  year = {2011},
  month = may,
  series = {{{ICSE}} '11},
  pages = {171--180},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1985793.1985817},
  urldate = {2023-10-10},
  abstract = {Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects - Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages.},
  isbn = {978-1-4503-0445-0},
  keywords = {developer productivity,empirical studies,high-level languages,software evolution,software quality},
  file = {/home/jsimonrichard/Zotero/storage/CFQNYPY4/Bhattacharya and Neamtiu - 2011 - Assessing programming language impact on developme.pdf}
}

@misc{binstockOracleBrandVoiceJavas,
  title = {Oracle {{BrandVoice}}: {{Java}}'s 20 {{Years Of Innovation}}},
  shorttitle = {Oracle {{BrandVoice}}},
  author = {Binstock, Andrew},
  journal = {Forbes},
  urldate = {2023-10-17},
  abstract = {Two decades of large-scale investment in Java's developing language and platform is strikingly rare in the history of languages.},
  chapter = {Tech},
  howpublished = {https://www.forbes.com/sites/oracle/2015/05/20/javas-20-years-of-innovation/},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/W65D538U/javas-20-years-of-innovation.html}
}

@article{borgwardtProteinFunctionPrediction2005,
  title = {Protein {{Function Prediction}} via {{Graph Kernels}}},
  author = {Borgwardt, Karsten and Ong, Cheng Soon and Sch{\"o}nauer, Stefan and Vishwanathan, S and Smola, Alexander and Kr{\"o}ger, Peer},
  year = {2005},
  month = jul,
  journal = {Bioinformatics (Oxford, England)},
  volume = {21 Suppl 1},
  pages = {i47-56},
  doi = {10.1093/bioinformatics/bti1007},
  abstract = {Computational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs. Our graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively. More information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.}
}

@article{borgwardtProteinFunctionPrediction2005a,
  title = {Protein Function Prediction via Graph Kernels},
  author = {Borgwardt, Karsten M. and Ong, Cheng Soon and Sch{\"o}nauer, Stefan and Vishwanathan, S. V. N. and Smola, Alex J. and Kriegel, Hans-Peter},
  year = {2005},
  month = jun,
  journal = {Bioinformatics},
  volume = {21},
  number = {suppl\_1},
  pages = {i47-i56},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bti1007},
  urldate = {2024-05-04},
  abstract = {Motivation: Computational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.Results: Our graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.Availability: More information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.Contact: ~borgwardt@dbs.ifi.lmu.de},
  file = {/home/jsimonrichard/Zotero/storage/ZJZXG27Q/Borgwardt et al. - 2005 - Protein Function Prediction via Graph Kernels.pdf;/home/jsimonrichard/Zotero/storage/IHB7HUX5/202991.html}
}

@incollection{bruknerEssenceEntanglement2021,
  title = {The {{Essence}} of {{Entanglement}}},
  booktitle = {Fundamental {{Theories}} of {{Physics}}},
  author = {Brukner, {\v C}aslav and {\.Z}ukowski, Marek and Zeilinger, Anton},
  year = {2021},
  volume = {203},
  issn = {23656425},
  doi = {10.1007/978-3-030-77367-0_6},
  abstract = {Entanglement, according to Erwin Schr{\"o}dinger the essence of quantum mechanics, is at the heart of the Einstein-Podolsky-Rosen paradox and of the so-called ``quantum nonlocality"---the fact that a local realistic explanation of quantum mechanics is not possible as quantitatively expressed by violation of Bell's inequalities. Even as entanglement gains increasing importance in most quantum information processing protocols, its conceptual foundation is still widely debated. Among the open questions are: What is the conceptual meaning of quantum entanglement? What are the most general constraints imposed by local realism? Which general quantum states violate these constraints? Developing Schr{\"o}dinger's ideas in an information-theoretic context, we suggest that a natural understanding of quantum entanglement results when one accepts (1) that the amount of information per elementary system is finite and (2) that the information in a composite system resides more in the correlations than in properties of individuals. The quantitative formulation of these ideas leads to a rather natural criterion of quantum entanglement for pure states, which starts from the amount of information in correlations, rather than the non-factorizability of Hilbert space vectors representing the states. Independently, extending Bell's original ideas, one can obtain a single general Bell inequality that summarizes all possible constraints imposed by local realism on the correlations for a multi-qubit system, and for situations in which the observers can choose between two (complementary) measurements. Violation of the general Bell inequality results in an independent general criterion for quantum entanglement for multi-qubit states. Most importantly, the two criteria agree in essence, though the two approaches are conceptually very different. This concurrence supports our information-theoretic interpretation of quantum entanglement{\textbar} and of quantum physics in general.}
}

@article{bruzewiczTrappedionQuantumComputing2019,
  title = {Trapped-Ion Quantum Computing: {{Progress}} and Challenges},
  author = {Bruzewicz, Colin D and Chiaverini, John and McConnell, Robert and Sage, Jeremy M},
  year = {2019},
  journal = {Applied Physics Reviews},
  volume = {6},
  number = {2},
  pages = {021314},
  publisher = {AIP Publishing LLC}
}

@article{buchmanWhatAreBiggest2022,
  title = {What Are the Biggest Threats to {{US}} National Security?},
  author = {Buchman, Cassie},
  year = {2022},
  month = aug,
  journal = {News Nation},
  urldate = {2022-11-23}
}

@misc{bugdenRustProgrammingLanguage2022,
  title = {Rust: {{The Programming Language}} for {{Safety}} and {{Performance}}},
  shorttitle = {Rust},
  author = {Bugden, William and Alahmar, Ayman},
  year = {2022},
  month = jun,
  journal = {arXiv.org},
  urldate = {2023-10-10},
  abstract = {Rust is a young programming language gaining increased attention from software developers since it was introduced to the world by Mozilla in 2010. In this study, we attempt to answer several research questions. Does Rust deserve such increased attention? What is there in Rust that is attracting programmers to this new language? Safety and performance were among the very first promises of Rust, as was claimed by its early developers. Is Rust a safe language with high performance? Have these claims been achieved? To answer these questions, we surveyed and analyzed recent research on Rust and research that benchmarks Rust with other available prominent programming languages. The results show that Rust deserves the increased interest by programmers, and recent experimental results in benchmarking research show Rust's overall superiority over other well-established languages in terms of performance, safety, and security. Even though this study was not comprehensive (and more work must be done in this area), it informs the programming and research communities on the promising features of Rust as the language of choice for the future.},
  howpublished = {https://arxiv.org/abs/2206.05503v1},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/3YDVKFA2/Bugden and Alahmar - 2022 - Rust The Programming Language for Safety and Perf.pdf}
}

@inproceedings{burstinghaus-steinbachPostQuantumTLSEmbedded2020,
  title = {Post-{{Quantum TLS}} on {{Embedded Systems}}: {{Integrating}} and {{Evaluating Kyber}} and {{SPHINCS}}+ with Mbed {{TLS}}},
  booktitle = {Proceedings of the 15th {{ACM Asia Conference}} on {{Computer}} and {{Communications Security}}, {{ASIA CCS}} 2020},
  author = {{B{\"u}rstinghaus-Steinbach}, Kevin and Krau{\ss}, Christoph and Niederhagen, Ruben and Schneider, Michael},
  year = {2020},
  doi = {10.1145/3320269.3384725},
  abstract = {We present our integration of post-quantum cryptography (PQC), more specifically of the post-quantum KEM scheme Kyber for key establishment and the post-quantum signature scheme SPHINCS+, into the embedded TLS library mbed TLS. We measure the performance of these post-quantum primitives on four different embedded platforms with three different ARM processors and an Xtensa LX6 processor. Furthermore, we compare the performance of our experimental PQC cipher suite to a classical TLS variant using elliptic curve cryptography (ECC). Post-quantum key establishment and signature schemes have been either integrated into TLS or ported to embedded devices before. However, to the best of our knowledge, we are the first to combine TLS, post-quantum schemes, and embedded systems and to measure and evaluate the performance of post-quantum TLS on embedded platforms. Our results show that post-quantum key establishment with Kyber performs well in TLS on embedded devices compared to ECC variants. The use of SPHINCS+ signatures comes with certain challenges in terms of signature size and signing time, which mainly affects the use of embedded systems as PQC-TLS server but does not necessarily prevent embedded systems to act as PQC-TLS clients.}
}

@article{castelvecchiQuantumcomputingPioneerWarns2020,
  title = {Quantum-Computing Pioneer Warns of Complacency over {{Internet}} Security},
  author = {Castelvecchi, Davide},
  year = {2020},
  journal = {Nature},
  volume = {587},
  number = {7833},
  issn = {14764687},
  doi = {10.1038/d41586-020-03068-9}
}

@article{Celebrating40yearAnniversary2021,
  title = {Celebrating the 40-Year Anniversary of the {{Physics}} of {{Computation Conference}}},
  year = {2021},
  month = mar,
  journal = {IBM Research},
  urldate = {2022-12-03}
}

@article{ceroniQuantumGraphRecurrent2020,
  title = {The {{Quantum Graph Recurrent Neural Network}}},
  author = {Ceroni, Jack},
  year = {2020},
  month = jul,
  journal = {PennyLane Demos},
  publisher = {Xanadu},
  urldate = {2023-12-22},
  abstract = {Using a quantum graph recurrent neural network to learn quantum dynamics.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/UGP2SXPW/tutorial_qgrnn.html}
}

@article{chBigGlitchNuclear1999,
  title = {Big {{Glitch}} at {{Nuclear Plant Shows Perils}} of {{Y2K Tests}}},
  author = {Ch, Rajiv and {rasekaran}},
  year = {1999},
  month = mar,
  journal = {The Washington Post},
  urldate = {2022-12-05}
}

@misc{chenNovelArchitectureParameterized2022,
  title = {Novel {{Architecture}} of {{Parameterized Quantum Circuit}} for {{Graph Convolutional Network}}},
  author = {Chen, Yanhu and Wang, Cen and Guo, Hongxiang and Jianwu},
  year = {2022},
  month = mar,
  number = {arXiv:2203.03251},
  eprint = {2203.03251},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.03251},
  urldate = {2024-01-20},
  abstract = {Recently, the implementation of quantum neural networks is based on noisy intermediate-scale quantum (NISQ) devices. Parameterized quantum circuit (PQC) is such the method, and its current design just can handle linear data classification. However, data in the real world often shows a topological structure. In the machine learning field, the classical graph convolutional layer (GCL)-based graph convolutional network (GCN) can well handle the topological data. Inspired by the architecture of a classical GCN, in this paper, to expand the function of the PQC, we design a novel PQC architecture to realize a quantum GCN (QGCN). More specifically, we first implement an adjacent matrix based on linear combination unitary and a weight matrix in a quantum GCL, and then by stacking multiple GCLs, we obtain the QGCN. In addition, we first achieve gradients decent on quantum circuit following the parameter-shift rule for a GCL and then for the QGCN. We evaluate the performance of the QGCN by conducting a node classification task on Cora dataset with topological data. The numerical simulation result shows that QGCN has the same performance as its classical counterpart, the GCN, in contrast, requires less tunable parameters. Compared to a traditional PQC, we also verify that deploying an extra adjacent matrix can significantly improve the classification performance for quantum topological data.},
  archiveprefix = {arXiv},
  keywords = {Cora Dataset,Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/595KYQG2/Chen et al. - 2022 - Novel Architecture of Parameterized Quantum Circui.pdf;/home/jsimonrichard/Zotero/storage/ZCKETAF3/2203.html}
}

@article{cheungDesignOptimizationQuantum2007,
  title = {On the {{Design}} and {{Optimization}} of a {{Quantum Polynomial-Time Attack}} on {{Elliptic Curve Cryptography}}},
  author = {Cheung, Donny and Maslov, Dmitri and Mathew, Jimson and Pradhan, Dhiraj K.},
  year = {2007},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.0710.1093},
  keywords = {FOS: Physical sciences,Quantum Physics (quant-ph)}
}

@inproceedings{cheungDesignOptimizationQuantum2008,
  title = {On the Design and Optimization of a Quantum Polynomial-Time Attack on Elliptic Curve Cryptography},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Cheung, Donny and Maslov, Dmitri and Mathew, Jimson and Pradhan, Dhiraj K.},
  year = {2008},
  volume = {5106 LNCS},
  issn = {16113349},
  doi = {10.1007/978-3-540-89304-2_9},
  abstract = {We consider a quantum polynomial-time algorithm which solves the discrete logarithm problem for points on elliptic curves over GF(2 m ). We improve over earlier algorithms by constructing an efficient circuit for multiplying elements of binary finite fields and by representing elliptic curve points using a technique based on projective coordinates. The depth of our proposed implementation is O(m 2), which is an improvement over the previous bound of O(m 3). {\copyright} 2008 Springer Berlin Heidelberg.}
}

@article{choiAIFusesQuantum2022,
  title = {{{AI Fuses With Quantum Computing}} in {{Promising New Memristor}}},
  author = {Choi, Charles Q.},
  year = {2022},
  month = apr,
  journal = {IEEE Spectrum},
  urldate = {2022-12-01}
}

@inproceedings{choiTutorialQuantumGraph2021,
  title = {A {{Tutorial}} on {{Quantum Graph Recurrent Neural Network}} ({{QGRNN}})},
  booktitle = {2021 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
  author = {Choi, Jaeho and Oh, Seunghyeok and Kim, Joongheon},
  year = {2021},
  month = jan,
  pages = {46--49},
  issn = {1976-7684},
  doi = {10.1109/ICOIN50884.2021.9333917},
  urldate = {2023-12-22},
  abstract = {Over the past decades, various neural networks have been proposed with the rapid development of the machine learning field. In particular, graph neural networks using feature-vectors assigned to nodes and edges have been attracting attention in various fields. The usefulness of graph neural networks also affected the field of quantum computing, which led to the birth of quantum graph neural networks composed of parameterized quantum circuits. The quantum graph neural networks have many possibilities as applications from the simulation perspective of quantum dynamics. Among the application models of various quantum graph neural networks, the quantum graph recurrent neural network (QGRNN) is proven to be effective in training the Ising model Hamiltonian. Thus, this paper introduces the concepts of the Ising model, variational quantum eigensolver (VQE) for preparing quantum data, and QGRNN from a software engineer's point of view.},
  file = {/home/jsimonrichard/Zotero/storage/U646WPJB/Choi et al. - 2021 - A Tutorial on Quantum Graph Recurrent Neural Netwo.pdf;/home/jsimonrichard/Zotero/storage/3ASNKAR5/references.html}
}

@article{choiTwoWorldsBiggest2021,
  title = {Two of {{World}}'s {{Biggest Quantum Computers Made}} in {{China}}},
  author = {Choi, Charles},
  year = {2021},
  month = nov,
  journal = {IEEE Spectrum},
  urldate = {2022-11-23}
}

@article{chuangExperimentalImplementationFast1998,
  title = {Experimental {{Implementation}} of {{Fast Quantum Searching}}},
  author = {Chuang, Isaac L. and Gershenfeld, Neil A. and Kubinec, Mark},
  year = {1998},
  journal = {Physical Review Letters},
  volume = {80},
  pages = {3408--3411}
}

@article{ciesielskiSierpinskisTopologicalCharacterization2020,
  title = {Sierpinski's {{Topological Characterization}} of {{$\mathbb{Q}$}}},
  author = {Ciesielski, Krzysztof Chris},
  year = {2020},
  journal = {Mathematics Magazine},
  volume = {93},
  number = {2},
  eprint = {48665711},
  eprinttype = {jstor},
  pages = {136--138},
  publisher = {[Mathematical Association of America, Taylor \& Francis, Ltd.]},
  issn = {0025-570X},
  urldate = {2024-04-13},
  abstract = {In a 1920 paper, Sierpinski proved the following theorem characterizing the space Q of rational numbers considered with the standard topology: Any countable metric space {$<$} X, d {$>$} without isolated points is homeomorphic to {$\mathbb{Q}$}. In this note, we provide a simple proof of this result, that requires only basic topological background. As such, it can be incorporated into an undergraduate topology curriculum.},
  file = {/home/jsimonrichard/Zotero/storage/QSM6CR5H/Ciesielski - 2020 - Sierpinski’s Topological Characterization of ℚ.pdf}
}

@article{collisPhysicsSimulationQuantum2023,
  title = {Physics Simulation via Quantum Graph Neural Network},
  author = {Collis, Benjamin and Patel, Saahil and Koch, Daniel and Cutugno, Massimiliano and Wessing, Laura and Alsing, Paul M.},
  year = {2023},
  month = apr,
  journal = {AVS Quantum Science},
  volume = {5},
  number = {2},
  pages = {023801},
  issn = {2639-0213},
  doi = {10.1116/5.0145722},
  urldate = {2024-01-21},
  abstract = {We develop and implement two realizations of quantum graph neural networks (QGNN), applied to the task of particle interaction simulation. The first QGNN is a speculative quantum-classical hybrid learning model that relies on the ability to directly utilize superposition states as classical information to propagate information between particles. The second is an implementable quantum-classical hybrid learning model that propagates particle information directly through the parameters of RX rotation gates. A classical graph neural network (CGNN) is also trained in the same task. Both the Speculative QGNN and CGNN act as controls against the Implementable QGNN. Comparison between classical and quantum models is based on the loss value and accuracy of each model. Overall, each model had a high learning efficiency, in which the loss value rapidly approached zero during training; however, each model was moderately inaccurate. Comparing performances, our results show that the Implementable QGNN has a potential advantage over the CGNN. Additionally, we show that a slight alteration in hyperparameters in the CGNN notably improves accuracy, suggesting that further fine tuning could mitigate the issue of moderate inaccuracy in each model.},
  file = {/home/jsimonrichard/Zotero/storage/JF2FR4RK/Collis et al. - 2023 - Physics simulation via quantum graph neural networ.pdf;/home/jsimonrichard/Zotero/storage/TWVZIMMT/2887192.html}
}

@article{daleyPracticalQuantumAdvantage2022,
  title = {Practical Quantum Advantage in Quantum Simulation},
  author = {Daley, Andrew J. and Bloch, Immanuel and Kokail, Christian and Flannigan, Stuart and Pearson, Natalie and Troyer, Matthias and Zoller, Peter},
  year = {2022},
  month = jul,
  journal = {Nature},
  volume = {607},
  number = {7920},
  pages = {667--676},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-04940-6}
}

@article{dallaire-demersQuantumGenerativeAdversarial2018,
  title = {Quantum Generative Adversarial Networks},
  author = {{Dallaire-Demers}, Pierre-Luc and Killoran, Nathan},
  year = {2018},
  month = jul,
  journal = {Physical Review A},
  volume = {98},
  number = {1},
  pages = {012324},
  issn = {2469-9926, 2469-9934},
  doi = {10.1103/PhysRevA.98.012324},
  urldate = {2023-09-30},
  abstract = {Quantum machine learning is expected to be one of the first potential general-purpose applications of near-term quantum devices. A major recent breakthrough in classical machine learning is the notion of generative adversarial training, where the gradients of a discriminator model are used to train a separate generative model. In this work and a companion paper, we extend adversarial training to the quantum domain and show how to construct generative adversarial networks using quantum circuits. Furthermore, we also show how to compute gradients -- a key element in generative adversarial network training -- using another quantum circuit. We give an example of a simple practical circuit ansatz to parametrize quantum machine learning models and perform a simple numerical experiment to demonstrate that quantum generative adversarial networks can be trained successfully.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/LQPBUCCX/Dallaire-Demers and Killoran - 2018 - Quantum generative adversarial networks.pdf}
}

@article{debFastElitistMultiobjective2002,
  title = {A Fast and Elitist Multiobjective Genetic Algorithm: {{NSGA-II}}},
  shorttitle = {A Fast and Elitist Multiobjective Genetic Algorithm},
  author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year = {2002},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {6},
  number = {2},
  pages = {182--197},
  issn = {1941-0026},
  doi = {10.1109/4235.996017},
  urldate = {2024-07-08},
  abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN/sup 3/) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN/sup 2/) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed.},
  keywords = {Associate members,Computational complexity,Computational modeling,Constraint optimization,Decision making,Diversity reception,Evolutionary computation,Genetic algorithms,Sorting,Testing},
  file = {/home/jsimonrichard/Zotero/storage/6UFE4AAZ/Deb et al. - 2002 - A fast and elitist multiobjective genetic algorith.pdf}
}

@misc{decaoMolGANImplicitGenerative2022,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  year = {2022},
  month = sep,
  number = {arXiv:1805.11973},
  eprint = {1805.11973},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.11973},
  urldate = {2023-10-26},
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse. Code at https://github.com/nicola-decao/MolGAN},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/S82TSYC9/De Cao and Kipf - 2022 - MolGAN An implicit generative model for small mol.pdf;/home/jsimonrichard/Zotero/storage/TK77IBAH/1805.html}
}

@misc{DesignCriteriaProgramming,
  title = {Design {{Criteria}} for {{Programming Languages}}},
  urldate = {2023-10-15},
  howpublished = {https://jcsites.juniata.edu/faculty/rhodes/lt/plcriteria.htm},
  file = {/home/jsimonrichard/Zotero/storage/6RLUJPSH/plcriteria.html}
}

@misc{dettmersQLoRAEfficientFinetuning2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  number = {arXiv:2305.14314},
  eprint = {2305.14314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14314},
  urldate = {2023-09-17},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/P7NI6T22/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf;/home/jsimonrichard/Zotero/storage/7JEWQIEW/2305.html}
}

@book{diracPrinciplesQuantumMechanics1981,
  title = {The Principles of Quantum Mechanics},
  author = {Dirac, Paul Adrien Maurice},
  year = {1981},
  number = {27},
  publisher = {Oxford university press}
}

@article{diverInterpretingRulesCode2021,
  title = {Interpreting the {{Rule}}(s) of {{Code}}: {{Performance}}, {{Performativity}}, and {{Production}}},
  shorttitle = {Interpreting the {{Rule}}(s) of {{Code}}},
  author = {Diver, Laurence},
  year = {2021},
  month = dec,
  journal = {MIT Computational Law Report},
  urldate = {2023-10-10},
  abstract = {Software code is built on rules. The way it enforces them is analogous in certain ways to the philosophical notion of legalism, under which citizens are expected to follow legal rules without thinking too hard about their meaning or consequences. By analogy, the opacity, immutability, immediacy, pervasiveness, private production, and `ruleishness' of code amplify its `legalistic' nature far beyond what could ever be imposed in the legal domain, however, raising significant questions about its legitimacy as a regulator. With the aim of mitigating this `computational legalism', the article explores how we might critically engage with the text of code, rather than just the effects of its execution. This means contrasting the technical performance of code with the social performativity of law, demonstrating the limits of viewing the latter as merely a regulative `modality' that can be easily supplanted by code. The latter part of the article considers code and the processes and tools of its production, drawing on theories of textual interpretation, linguistics, and critical code studies to consider how its production might be legitimised.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/CJNCEF6S/Diver - 2021 - Interpreting the Rule(s) of Code Performance, Per.pdf}
}

@article{dobsonDistinguishingEnzymeStructures2003,
  title = {Distinguishing {{Enzyme Structures}} from {{Non-enzymes Without Alignments}}},
  author = {Dobson, Paul D. and Doig, Andrew J.},
  year = {2003},
  month = jul,
  journal = {Journal of Molecular Biology},
  volume = {330},
  number = {4},
  pages = {771--783},
  issn = {0022-2836},
  doi = {10.1016/S0022-2836(03)00628-4},
  urldate = {2024-05-04},
  abstract = {The ability to predict protein function from structure is becoming increasingly important as the number of structures resolved is growing more rapidly than our capacity to study function. Current methods for predicting protein function are mostly reliant on identifying a similar protein of known function. For proteins that are highly dissimilar or are only similar to proteins also lacking functional annotations, these methods fail. Here, we show that protein function can be predicted as enzymatic or not without resorting to alignments. We describe 1178 high-resolution proteins in a structurally non-redundant subset of the Protein Data Bank using simple features such as secondary-structure content, amino acid propensities, surface properties and ligands. The subset is split into two functional groupings, enzymes and non-enzymes. We use the support vector machine-learning algorithm to develop models that are capable of assigning the protein class. Validation of the method shows that the function can be predicted to an accuracy of 77\% using 52 features to describe each protein. An adaptive search of possible subsets of features produces a simplified model based on 36 features that predicts at an accuracy of 80\%. We compare the method to sequence-based methods that also avoid calculating alignments and predict a recently released set of unrelated proteins. The most useful features for distinguishing enzymes from non-enzymes are secondary-structure content, amino acid frequencies, number of disulphide bonds and size of the largest cleft. This method is applicable to any structure as it does not require the identification of sequence or structural similarity to a protein of known function.},
  keywords = {enzyme,machine learning,protein function prediction,structural genomics,structure},
  file = {/home/jsimonrichard/Zotero/storage/6YDJC6WZ/Dobson and Doig - 2003 - Distinguishing Enzyme Structures from Non-enzymes .pdf;/home/jsimonrichard/Zotero/storage/G2GH3QK3/S0022283603006284.html}
}

@article{dunjkoMachineLearningArtificial2018,
  title = {Machine Learning \& Artificial Intelligence in the Quantum Domain: A Review of Recent Progress},
  shorttitle = {Machine Learning \& Artificial Intelligence in the Quantum Domain},
  author = {Dunjko, Vedran and Briegel, Hans J.},
  year = {2018},
  month = jun,
  journal = {Reports on Progress in Physics},
  volume = {81},
  number = {7},
  pages = {074001},
  publisher = {IOP Publishing},
  issn = {0034-4885},
  doi = {10.1088/1361-6633/aab406},
  urldate = {2024-03-07},
  abstract = {Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research---quantum information versus machine learning (ML) and artificial intelligence (AI)---have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our `big data' world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement---exploring what ML/AI can do for quantum physics and vice versa---researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/5DTUL6F8/Dunjko and Briegel - 2018 - Machine learning & artificial intelligence in the .pdf}
}

@article{dyakonovWhenWillUseful2019,
  title = {When Will Useful Quantum Computers Be Constructed? {{Not}} in the Foreseeable Future, This Physicist Argues. {{Here}}'s Why: {{The}} Case against: {{Quantum}} Computing},
  author = {Dyakonov, Mikhail},
  year = {2019},
  journal = {IEEE Spectrum},
  volume = {56},
  number = {3},
  issn = {19399340},
  doi = {10.1109/MSPEC.2019.8651931},
  abstract = {Quantum Computing IS ALL THE RAGE. It seems like hardly a day goes by without some news outlet describing the extraordinary things this technology promises. Most commentators forget, or just gloss over, the fact that people have been working on quantum computing for decades - and without any practical results to show for it. t IBM notes that quantum computers could 'provide breakthroughs in many disciplines, including materials and drug discovery, the optimization of complex systems, and artificial intelligence.' Microsoft assures us that quantum computers will 'forever alter our economic, industrial, academic, and societal landscape.' And journalists repeatedly warn that quantum computers may soon break the encryption that protects the Internet. It has gotten to the point where many researchers in various fields of physics feel obliged to justify whatever work they are doing by claiming that it has some relevance to quantum computing.}
}

@misc{FacebookresearchFvcore2024,
  title = {Facebookresearch/Fvcore},
  year = {2024},
  month = jul,
  urldate = {2024-07-20},
  abstract = {Collection of common code that's shared among different research projects in FAIR computer vision team.},
  copyright = {Apache-2.0},
  howpublished = {Meta Research}
}

@article{fernandez-caramesPreQuantumPostQuantumIoT2020,
  title = {From {{Pre-Quantum}} to {{Post-Quantum IoT Security}}: {{A Survey}} on {{Quantum-Resistant Cryptosystems}} for the {{Internet}} of {{Things}}},
  author = {{Fernandez-Carames}, Tiago M.},
  year = {2020},
  journal = {IEEE Internet of Things Journal},
  volume = {7},
  number = {7},
  issn = {23274662},
  doi = {10.1109/JIOT.2019.2958788},
  abstract = {Although quantum computing is still in its nascent age, its evolution threatens the most popular public-key encryption systems. Such systems are essential for today's Internet security due to their ability for solving the key distribution problem and for providing high security in insecure communications channels that allow for accessing websites or for exchanging e-mails, financial transactions, digitally signed documents, military communications or medical data. Cryptosystems like Rivest-Shamir-Adleman (RSA), elliptic curve cryptography (ECC) or Diffie-Hellman have spread worldwide and are part of diverse key Internet standards like Transport Layer Security (TLS), which are used both by traditional computers and Internet of Things (IoT) devices. It is especially difficult to provide high security to IoT devices, mainly because many of them rely on batteries and are resource constrained in terms of computational power and memory, which implies that specific energy-efficient and lightweight algorithms need to be designed and implemented for them. These restrictions become relevant challenges when implementing cryptosystems that involve intensive mathematical operations and demand substantial computational resources, which are often required in applications where data privacy has to be preserved for the long term, like IoT applications for defense, mission-critical scenarios or smart healthcare. Quantum computing threatens such a long-term IoT device security and researchers are currently developing solutions to mitigate such a threat. This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated. Thus, this article is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers.}
}

@inproceedings{feyFastGraphRepresentation2019,
  title = {Fast Graph Representation Learning with {{PyTorch Geometric}}},
  booktitle = {{{ICLR}} Workshop on Representation Learning on Graphs and Manifolds},
  author = {Fey, Matthias and Lenssen, Jan E.},
  year = {2019},
  file = {/home/jsimonrichard/Zotero/storage/3R25B7C8/Fey and Lenssen - 2019 - Fast Graph Representation Learning with PyTorch Ge.pdf;/home/jsimonrichard/Zotero/storage/KVK455TN/1903.html}
}

@article{feynmanSimulatingPhysicsComputers1982,
  title = {Simulating Physics with Computers},
  author = {Feynman, Richard P.},
  year = {1982},
  month = jun,
  journal = {International Journal of Theoretical Physics},
  volume = {21},
  number = {6},
  pages = {467--488},
  issn = {1572-9575},
  doi = {10.1007/BF02650179}
}

@misc{FiehnLabJAVA,
  title = {Fiehn {{Lab}} - {{JAVA}} vs {{C}}++ {{Benchmark}}},
  urldate = {2023-10-21},
  howpublished = {https://fiehnlab.ucdavis.edu/staff/kind/collector/benchmark/java-benchmark},
  file = {/home/jsimonrichard/Zotero/storage/ZNKDE4R6/java-benchmark.html}
}

@article{fongMarginalLikelihoodCrossvalidation2020,
  title = {On the Marginal Likelihood and Cross-Validation},
  author = {Fong, E and Holmes, C C},
  year = {2020},
  month = jun,
  journal = {Biometrika},
  volume = {107},
  number = {2},
  pages = {489--496},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz077},
  urldate = {2024-04-27},
  abstract = {In Bayesian statistics, the marginal likelihood, also known as the evidence, is used to evaluate model fit as it quantifies the joint probability of the data under the prior. In contrast, non-Bayesian models are typically compared using cross-validation on held-out data, either through \$k\$-fold partitioning or leave-\$p\$-out subsampling. We show that the marginal likelihood is formally equivalent to exhaustive leave-\$p\$-out crossvalidation averaged over all values of \$p\$ and all held-out test sets when using the log posterior predictive probability as the scoring rule. Moreover, the log posterior predictive score is the only coherent scoring rule under data exchangeability. This offers new insight into the marginal likelihood and cross-validation, and highlights the potential sensitivity of the marginal likelihood to the choice of the prior. We suggest an alternative approach using cumulative cross-validation following a preparatory training phase. Our work has connections to prequential analysis and intrinsic Bayes factors, but is motivated in a different way.},
  file = {/home/jsimonrichard/Zotero/storage/W6I2DZGI/Fong and Holmes - 2020 - On the marginal likelihood and cross-validation.pdf;/home/jsimonrichard/Zotero/storage/UW9L522G/5715611.html}
}

@article{gaoEnhancingGenerativeModels2022,
  title = {Enhancing {{Generative Models}} via {{Quantum Correlations}}},
  author = {Gao, Xun and Anschuetz, Eric R. and Wang, Sheng-Tao and Cirac, J. Ignacio and Lukin, Mikhail D.},
  year = {2022},
  month = may,
  journal = {Physical Review X},
  volume = {12},
  number = {2},
  pages = {021037},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.12.021037},
  urldate = {2023-10-07},
  abstract = {Generative modeling using samples drawn from the probability distribution constitutes a powerful approach for unsupervised machine learning. Quantum mechanical systems can produce probability distributions that exhibit quantum correlations which are difficult to capture using classical models. We show theoretically that such quantum-inspired correlations provide a powerful resource for generative modeling. In particular, we provide an unconditional proof of separation in expressive power between a class of widely used generative models, known as Bayesian networks, and its minimal quantum-inspired extension. We show that this expressivity enhancement is associated with quantum nonlocality and quantum contextuality. Furthermore, we numerically test this separation on standard machine-learning data sets and show that it holds for practical problems. The possibility of quantum-inspired enhancement demonstrated in this work not only sheds light on the design of useful quantum machine-learning protocols but also provides inspiration to draw on ideas from quantum foundations to improve purely classical algorithms.},
  file = {/home/jsimonrichard/Zotero/storage/7MRB95SR/Gao et al. - 2022 - Enhancing Generative Models via Quantum Correlatio.pdf;/home/jsimonrichard/Zotero/storage/NLDGMRMF/PhysRevX.12.html}
}

@article{gaoQuantumMachineLearning2018,
  title = {A Quantum Machine Learning Algorithm Based on Generative Models},
  author = {Gao, X. and Zhang, Z.-Y. and Duan, L.-M.},
  year = {2018},
  month = dec,
  journal = {Science Advances},
  volume = {4},
  number = {12},
  pages = {eaat9004},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.aat9004},
  urldate = {2023-09-11},
  abstract = {Quantum computing and artificial intelligence, combined together, may revolutionize future technologies. A significant school of thought regarding artificial intelligence is based on generative models. Here, we propose a general quantum algorithm for machine learning based on a quantum generative model. We prove that our proposed model is more capable of representing probability distributions compared with classical generative models and has exponential speedup in learning and inference at least for some instances if a quantum computer cannot be efficiently simulated classically. Our result opens a new direction for quantum machine learning and offers a remarkable example where a quantum algorithm shows exponential improvement over classical algorithms in an important application field.},
  keywords = {generative artificial intelligence,quantum machine learning},
  file = {/home/jsimonrichard/Zotero/storage/F8GLE3N2/Gao et al. - 2018 - A quantum machine learning algorithm based on gene.pdf;/home/jsimonrichard/Zotero/storage/KZS8SCVE/Supplamentary Materials PDF.pdf}
}

@article{gidneyHowFactor20482021,
  title = {How to Factor 2048 Bit {{RSA}} Integers in 8 Hours Using 20 Million Noisy Qubits},
  author = {Gidney, Craig and Eker{\aa}, Martin},
  year = {2021},
  journal = {Quantum},
  volume = {5},
  issn = {2521327X},
  doi = {10.22331/Q-2021-04-15-433},
  abstract = {We significantly reduce the cost of factoring integers and computing discrete logarithms in finite fields on a quantum computer by combining techniques from Shor 1994, Griffiths-Niu 1996, Zalka 2006, Fowler 2012, Eker{\aa}-H{\aa}stad 2017, Eker{\aa} 2017, Eker{\aa} 2018, Gidney-Fowler 2019, Gidney 2019. We estimate the approximate cost of our construction using plausible physical assumptions for large-scale superconducting qubit platforms: a planar grid of qubits with nearest-neighbor connectivity, a characteristic physical gate error rate of 10-3, a surface code cycle time of 1 microsecond, and a reaction time of 10 microseconds. We account for factors that are normally ignored such as noise, the need to make repeated attempts, and the spacetime layout of the computation. When factoring 2048 bit RSA integers, our construction's spacetime volume is a hundredfold less than comparable estimates from earlier works (Van Meter et al. 2009, Jones et al. 2010, Fowler et al. 2012, Gheorghiu et al. 2019). In the abstract circuit model (which ignores overheads from distillation, routing, and error correction) our construction uses 3n + 0.002n lg n logical qubits, 0.3n3 + 0.0005n3 lg n Toffolis, and 500n2 + n2 lg n measurement depth to factor n-bit RSA integers. We quantify the cryptographic implications of our work, both for RSA and for schemes based on the DLP in finite fields.}
}

@article{gillQuantumComputingTaxonomy2022,
  title = {Quantum Computing: {{A}} Taxonomy, Systematic Review and Future Directions},
  author = {Gill, Sukhpal Singh and Kumar, Adarsh and Singh, Harvinder and Singh, Manmeet and Kaur, Kamalpreet and Usman, Muhammad and Buyya, Rajkumar},
  year = {2022},
  journal = {Software - Practice and Experience},
  volume = {52},
  number = {1},
  issn = {1097024X},
  doi = {10.1002/spe.3039},
  abstract = {Quantum computing (QC) is an emerging paradigm with the potential to offer significant computational advantage over conventional classical computing by exploiting quantum-mechanical principles such as entanglement and superposition. It is anticipated that this computational advantage of QC will help to solve many complex and computationally intractable problems in several application domains such as drug design, data science, clean energy, finance, industrial chemical development, secure communications, and quantum chemistry. In recent years, tremendous progress in both quantum hardware development and quantum software/algorithm has brought QC much closer to reality. Indeed, the demonstration of quantum supremacy marks a significant milestone in the Noisy Intermediate Scale Quantum (NISQ) era---the next logical step being the quantum advantage whereby quantum computers solve a real-world problem much more efficiently than classical computing. As the quantum devices are expected to steadily scale up in the next few years, quantum decoherence and qubit interconnectivity are two of the major challenges to achieve quantum advantage in the NISQ era. QC is a highly topical and fast-moving field of research with significant ongoing progress in all facets. A systematic review of the existing literature on QC will be invaluable to understand the state-of-the-art of this emerging field and identify open challenges for the QC community to address in the coming years. This article presents a comprehensive review of QC literature and proposes taxonomy of QC. The proposed taxonomy is used to map various related studies to identify the research gaps. A detailed overview of quantum software tools and technologies, post-quantum cryptography, and quantum computer hardware development captures the current state-of-the-art in the respective areas. The article identifies and highlights various open challenges and promising future directions for research and innovation in QC.}
}

@article{goldsteinBellsTheorem2011,
  title = {Bell's Theorem},
  author = {Goldstein, S. and Norsen, T. and Tausk, D. Victor and Zanghi, N.},
  year = {2011},
  journal = {Scholarpedia},
  volume = {6},
  number = {10},
  pages = {8378},
  doi = {10.4249/scholarpedia.8378}
}

@article{guldeImplementationDeutschJozsa2003,
  title = {Implementation of the {{Deutsch}}--{{Jozsa}} Algorithm on an Ion-Trap Quantum Computer},
  author = {Gulde, Stephan and Riebe, Mark and Lancaster, Gavin P. T. and Becher, Christoph and Eschner, J{\"u}rgen and H{\"a}ffner, Hartmut and {Schmidt-Kaler}, Ferdinand and Chuang, Isaac L. and Blatt, Rainer},
  year = {2003},
  month = jan,
  journal = {Nature},
  volume = {421},
  number = {6918},
  pages = {48--50},
  issn = {1476-4687},
  doi = {10.1038/nature01336}
}

@article{gyongyosiSurveyQuantumComputing2019,
  title = {A {{Survey}} on Quantum Computing Technology},
  author = {Gyongyosi, Laszlo and Imre, Sandor},
  year = {2019},
  journal = {Computer Science Review},
  volume = {31},
  issn = {15740137},
  doi = {10.1016/j.cosrev.2018.11.002},
  abstract = {The power of quantum computing technologies is based on the fundamentals of quantum mechanics, such as quantum superposition, quantum entanglement, or the no-cloning theorem. Since these phenomena have no classical analogue, similar results cannot be achieved within the framework of traditional computing. The experimental insights of quantum computing technologies have already been demonstrated, and several studies are in progress. Here we review the most recent results of quantum computation technology and address the open problems of the field.},
  keywords = {Quantum computations,Quantum computer,Quantum entanglement,Quantum information processing}
}

@article{hamiltonGenerativeModelBenchmarks2019,
  title = {Generative Model Benchmarks for Superconducting Qubits},
  author = {Hamilton, Kathleen E. and Dumitrescu, Eugene F. and Pooser, Raphael C.},
  year = {2019},
  month = jun,
  journal = {Physical Review A},
  volume = {99},
  number = {6},
  pages = {062323},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.99.062323},
  urldate = {2023-10-07},
  abstract = {In this work we demonstrate experimentally how generative model training can be used as a benchmark for small (fewer than five qubits) quantum devices. Performance is quantified using three data analytic metrics: the Kullback-Leibler divergence and two adaptations of the F1 score. Using the 2{\texttimes}2 bars and stripes data set, we train several different circuit constructions for generative modeling with superconducting qubits. By taking hardware connectivity constraints into consideration, we show that sparsely connected shallow circuits outperform denser counterparts on noisy hardware.},
  file = {/home/jsimonrichard/Zotero/storage/MPKT6AV2/Hamilton et al. - 2019 - Generative model benchmarks for superconducting qu.pdf;/home/jsimonrichard/Zotero/storage/FHCB2GAV/PhysRevA.99.html}
}

@article{handsteinerCosmicBellTest2017,
  title = {Cosmic {{Bell Test}}: {{Measurement Settings}} from {{Milky Way Stars}}},
  author = {Handsteiner, Johannes and Friedman, Andrew S. and Rauch, Dominik and Gallicchio, Jason and Liu, Bo and Hosp, Hannes and Kofler, Johannes and Bricher, David and Fink, Matthias and Leung, Calvin and Mark, Anthony and Nguyen, Hien T. and Sanders, Isabella and Steinlechner, Fabian and Ursin, Rupert and Wengerowsky, S{\"o}ren and Guth, Alan H. and Kaiser, David I. and Scheidl, Thomas and Zeilinger, Anton},
  year = {2017},
  journal = {Physical Review Letters},
  volume = {118},
  number = {6},
  issn = {10797114},
  doi = {10.1103/PhysRevLett.118.060401},
  abstract = {Bell's theorem states that some predictions of quantum mechanics cannot be reproduced by a local-realist theory. That conflict is expressed by Bell's inequality, which is usually derived under the assumption that there are no statistical correlations between the choices of measurement settings and anything else that can causally affect the measurement outcomes. In previous experiments, this "freedom of choice" was addressed by ensuring that selection of measurement settings via conventional "quantum random number generators" was spacelike separated from the entangled particle creation. This, however, left open the possibility that an unknown cause affected both the setting choices and measurement outcomes as recently as mere microseconds before each experimental trial. Here we report on a new experimental test of Bell's inequality that, for the first time, uses distant astronomical sources as "cosmic setting generators." In our tests with polarization-entangled photons, measurement settings were chosen using real-time observations of Milky Way stars while simultaneously ensuring locality. Assuming fair sampling for all detected photons, and that each stellar photon's color was set at emission, we observe statistically significant 7.31{$\sigma$} and 11.93{$\sigma$} violations of Bell's inequality with estimated p values of 1.8{\texttimes}10-13 and 4.0{\texttimes}10-33, respectively, thereby pushing back by {$\sim$}600 years the most recent time by which any local-realist influences could have engineered the observed Bell violation.}
}

@article{haoChinaSeeksQuantum2022,
  title = {China {{Seeks}} a {{Quantum Leap}} in {{Computing}}},
  author = {Hao, Karen},
  year = {2022},
  month = oct,
  journal = {Wall Street Journal},
  urldate = {2022-11-24}
}

@article{hartnettNewLawDescribe2019,
  title = {A {{New Law}} to {{Describe Quantum Computing}}'s {{Rise}}?},
  author = {Hartnett, Kevin},
  year = {2019},
  month = jun,
  journal = {Quanta Magazine},
  urldate = {2022-11-23}
}

@book{hatcherAlgebraicTopology2001,
  title = {Algebraic {{Topology}}},
  author = {Hatcher, Allen},
  year = {2001},
  urldate = {2024-04-28},
  file = {/home/jsimonrichard/Zotero/storage/6A4ABRWU/AT+.pdf;/home/jsimonrichard/Zotero/storage/A34XJWA8/ATpage.html}
}

@article{haugQuantumMachineLearning2023,
  title = {Quantum Machine Learning of Large Datasets Using Randomized Measurements},
  author = {Haug, Tobias and Self, Chris N. and Kim, M. S.},
  year = {2023},
  month = jan,
  journal = {Machine Learning: Science and Technology},
  volume = {4},
  number = {1},
  pages = {015005},
  publisher = {IOP Publishing},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/acb0b4},
  urldate = {2023-10-07},
  abstract = {Quantum computers promise to enhance machine learning for practical applications. Quantum machine learning for real-world data has to handle extensive amounts of high-dimensional data. However, conventional methods for measuring quantum kernels are impractical for large datasets as they scale with the square of the dataset size. Here, we measure quantum kernels using randomized measurements. The quantum computation time scales linearly with dataset size and quadratic for classical post-processing. While our method scales in general exponentially in qubit number, we gain a substantial speed-up when running on intermediate-sized quantum computers. Further, we efficiently encode high-dimensional data into quantum computers with the number of features scaling linearly with the circuit depth. The encoding is characterized by the quantum Fisher information metric and is related to the radial basis function kernel. Our approach is robust to noise via a cost-free error mitigation scheme. We demonstrate the advantages of our methods for noisy quantum computers by classifying images with the IBM quantum computer. To achieve further speedups we distribute the quantum computational tasks between different quantum computers. Our method enables benchmarking of quantum machine learning algorithms with large datasets on currently available quantum computers.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/6DR8NX3M/Haug et al. - 2023 - Quantum machine learning of large datasets using r.pdf}
}

@article{heaneyExtremeNonlocalityOne2011,
  title = {Extreme Nonlocality with One Photon},
  author = {Heaney, Libby and Cabello, Ad{\'a}n and Santos, Marcelo Fran{\c c}a and Vedral, Vlatko},
  year = {2011},
  journal = {New Journal of Physics},
  volume = {13},
  issn = {13672630},
  doi = {10.1088/1367-2630/13/5/053054},
  abstract = {Quantum nonlocality is typically assigned to systems of two or more well-separated particles, but nonlocality can also exist in systems consisting of just a single particle when one considers the subsystems to be distant spatial field modes. Single particle nonlocality has been confirmed experimentally via a bipartite Bell inequality. In this paper, we introduce an N-party Hardy-like proof of the impossibility of local elements of reality and a Bell inequality for local realistic theories in the case of a single particle superposed symmetrically over N spatial field modes (i.e. N qubit W state). We show that, in the limit of large N, the Hardy-like proof effectively becomes an all-versus-nothing (or Greenberger-Horne-Zeilinger (GHZ)-like) proof, and the quantum-classical gap of the Bell inequality tends to be the same as that in a three-particle GHZ experiment. We describe how to test the nonlocality in realistic systems. {\copyright} IOP Publishing Ltd and Deutsche Physikalische Gesellschaft.}
}

@inproceedings{hekkalaImplementingPostquantumCryptography2022,
  title = {Implementing {{Post-quantum Cryptography}} for {{Developers}}},
  author = {Hekkala, Julius and Halunen, Kimmo and Vallivaara, Visa},
  year = {2022},
  doi = {10.5220/0010786200003120},
  abstract = {The possibility of a quantum computer threatens modern public key cryptography. Post-quantum cryptographic algorithms are designed to protect sensitive data and communications also against an attacker equipped with a quantum computer. National Institute of Standards and Technology is standardizing post-quantum algorithms that could replace currently used public key cryptographic algorithms in key exchange and digital signatures. Lattice-based cryptography is one of the post-quantum algorithm groups with the biggest potential. Cryptography libraries are used by developers in all kinds of different solutions, but currently the availability of post-quantum algorithms in open-source libraries is very limited. Implementing post-quantum algorithms into a software library involves a multitude of challenges. We integrated three lattice-based post-quantum algorithms into a fork of Crypto++, a C++ cryptography library. We analyzed challenges in the implementation process and the performance and security of the fork. Especially the complex mathematical ideas behind the algorithms make implementation difficult. The performance of the algorithms was satisfactory but analyzing the security of the implementation in more detail is needed.}
}

@book{heymanComparisonPerformanceAmpamp2020,
  title = {A {{Comparison}} of {{Performance}} \&amp;Amp; {{Implementation Complexity}} of {{Multithreaded Applications}} in {{Rust}}, {{Java}} and {{C}}++},
  author = {Heyman, Hugo and Brandefelt, Love},
  year = {2020},
  urldate = {2023-10-10},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/83J7GFAK/Heyman and Brandefelt - 2020 - A Comparison of Performance &amp;amp; Implementati.pdf}
}

@incollection{hoareHintsProgrammingLanguage1983,
  title = {Hints on {{Programming Language Design}}},
  booktitle = {Programming {{Languages}}},
  author = {Hoare, C. a. R.},
  editor = {Horowitz, Ellis},
  year = {1983},
  pages = {31--40},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-09507-2_3},
  urldate = {2023-10-10},
  isbn = {978-3-662-09509-6 978-3-662-09507-2},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/SFMX54HA/Hoare - 1983 - Hints on Programming Language Design.pdf}
}

@misc{hoareNullReferencesBillion,
  title = {Null {{References}}: {{The Billion Dollar Mistake}}},
  shorttitle = {Null {{References}}},
  author = {Hoare, C. a. R.},
  journal = {InfoQ},
  urldate = {2023-11-24},
  abstract = {Tony Hoare introduced Null references in ALGOL W back in 1965 "simply because it was so easy to implement", says Mr. Hoare. He talks about that decision considering it "my billion-dollar mistake".},
  howpublished = {https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/VFFCFWDM/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare.html}
}

@article{hoareQualitySoftware1972,
  title = {The Quality of Software},
  author = {Hoare, C. a. R.},
  year = {1972},
  journal = {Software: Practice and Experience},
  volume = {2},
  number = {2},
  pages = {103--105},
  issn = {1097-024X},
  doi = {10.1002/spe.4380020202},
  urldate = {2023-10-10},
  copyright = {Copyright {\copyright} 1972 John Wiley \& Sons, Ltd},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/4XILU6MG/Hoare - 1972 - The quality of software.pdf;/home/jsimonrichard/Zotero/storage/H5HG23W2/spe.html}
}

@article{holmesConnectingAnsatzExpressibility2022,
  title = {Connecting Ansatz Expressibility to Gradient Magnitudes and Barren Plateaus},
  author = {Holmes, Zo{\"e} and Sharma, Kunal and Cerezo, M. and Coles, Patrick J.},
  year = {2022},
  month = jan,
  journal = {PRX Quantum},
  volume = {3},
  number = {1},
  eprint = {2101.02138},
  primaryclass = {quant-ph, stat},
  pages = {010313},
  issn = {2691-3399},
  doi = {10.1103/PRXQuantum.3.010313},
  urldate = {2023-09-30},
  abstract = {Parameterized quantum circuits serve as ans{\textbackslash}"\{a\}tze for solving variational problems and provide a flexible paradigm for programming near-term quantum computers. Ideally, such ans{\textbackslash}"\{a\}tze should be highly expressive so that a close approximation of the desired solution can be accessed. On the other hand, the ansatz must also have sufficiently large gradients to allow for training. Here, we derive a fundamental relationship between these two essential properties: expressibility and trainability. This is done by extending the well established barren plateau phenomenon, which holds for ans{\textbackslash}"\{a\}tze that form exact 2-designs, to arbitrary ans{\textbackslash}"\{a\}tze. Specifically, we calculate the variance in the cost gradient in terms of the expressibility of the ansatz, as measured by its distance from being a 2-design. Our resulting bounds indicate that highly expressive ans{\textbackslash}"\{a\}tze exhibit flatter cost landscapes and therefore will be harder to train. Furthermore, we provide numerics illustrating the effect of expressiblity on gradient scalings, and we discuss the implications for designing strategies to avoid barren plateaus.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantum Physics,Statistics - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/TE63UJ7F/Holmes et al. - 2022 - Connecting ansatz expressibility to gradient magni.pdf;/home/jsimonrichard/Zotero/storage/XNT4X5TZ/2101.html}
}

@article{housseinMachineLearningQuantum2022,
  title = {Machine Learning in the Quantum Realm: {{The}} State-of-the-Art, Challenges, and Future Vision},
  shorttitle = {Machine Learning in the Quantum Realm},
  author = {Houssein, Essam H. and Abohashima, Zainab and Elhoseny, Mohamed and Mohamed, Waleed M.},
  year = {2022},
  month = may,
  journal = {Expert Systems with Applications},
  volume = {194},
  pages = {116512},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.116512},
  urldate = {2023-09-14},
  abstract = {Machine learning has become a ubiquitous and effective technique for data processing and classification. Furthermore, due to the superiority and progress of quantum computing in many areas (e.g., cryptography, machine learning, healthcare), a combination of classical machine learning and quantum information processing has established a new field, called, quantum machine learning. One of the most frequently used applications of quantum computing is machine learning. This paper aims to present a comprehensive review of state-of-the-art advances in quantum machine learning. Besides, this paper outlines recent works on different architectures of quantum deep learning, and illustrates classification tasks in the quantum domain as well as encoding methods and quantum subroutines. Furthermore, this paper examines how the concept of quantum computing enhances classical machine learning. Two methods for improving the performance of classical machine learning are presented. Finally, this work provides a general review of challenges and the future vision of quantum machine learning.},
  keywords = {Hybrid quantum-classical,Quantum classification,Quantum computing,Quantum deep learning,Quantum inspired,Quantum machine learning,Variational quantum algorithms},
  file = {/home/jsimonrichard/Zotero/storage/CGCTF4LU/Houssein et al. - 2022 - Machine learning in the quantum realm The state-o.pdf}
}

@misc{HowRustWent,
  title = {How {{Rust}} Went from a Side Project to the World's Most-Loved Programming Language},
  journal = {MIT Technology Review},
  urldate = {2023-10-17},
  abstract = {For decades, coders wrote critical systems in C and C++. Now they turn to Rust.},
  howpublished = {https://www.technologyreview.com/2023/02/14/1067869/rust-worlds-fastest-growing-programming-language/},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/AG67E7BC/rust-worlds-fastest-growing-programming-language.html}
}

@article{huangExperimentalQuantumGenerative2021,
  title = {Experimental {{Quantum Generative Adversarial Networks}} for {{Image Generation}}},
  author = {Huang, He-Liang and Du, Yuxuan and Gong, Ming and Zhao, Youwei and Wu, Yulin and Wang, Chaoyue and Li, Shaowei and Liang, Futian and Lin, Jin and Xu, Yu and Yang, Rui and Liu, Tongliang and Hsieh, Min-Hsiu and Deng, Hui and Rong, Hao and Peng, Cheng-Zhi and Lu, Chao-Yang and Chen, Yu-Ao and Tao, Dacheng and Zhu, Xiaobo and Pan, Jian-Wei},
  year = {2021},
  month = aug,
  journal = {Physical Review Applied},
  volume = {16},
  number = {2},
  pages = {024051},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevApplied.16.024051},
  urldate = {2023-09-30},
  abstract = {Quantum machine learning is expected to be one of the first practical applications of near-term quantum devices. Pioneer theoretical works suggest that quantum generative adversarial networks (GANs) may exhibit a potential exponential advantage over classical GANs, thus attracting widespread attention. However, it remains elusive whether quantum GANs implemented on near-term quantum devices can actually solve real-world learning tasks. Here, we devise a flexible quantum GAN scheme to narrow this knowledge gap. In principle, this scheme has the ability to complete image generation with high-dimensional features and could harness quantum superposition to train multiple examples in parallel. We experimentally achieve the learning and generating of real-world handwritten digit images on a superconducting quantum processor. Moreover, we utilize a gray-scale bar dataset to exhibit competitive performance between quantum GANs and the classical GANs based on multilayer perceptron and convolutional neural network architectures, respectively, benchmarked by the Fr{\'e}chet distance score. Our work provides guidance for developing advanced quantum generative models on near-term quantum devices and opens up an avenue for exploring quantum advantages in various GAN-related learning tasks.},
  keywords = {GAN,has code,quantum machine learning},
  file = {/home/jsimonrichard/Zotero/storage/JTSYDZNJ/Huang et al. - 2021 - Experimental Quantum Generative Adversarial Networ.pdf;/home/jsimonrichard/Zotero/storage/UQZUW3KC/PhysRevApplied.16.html}
}

@article{huangTermQuantumComputing2023,
  title = {Near-Term Quantum Computing Techniques: {{Variational}} Quantum Algorithms, Error Mitigation, Circuit Compilation, Benchmarking and Classical Simulation},
  shorttitle = {Near-Term Quantum Computing Techniques},
  author = {Huang, He-Liang and Xu, Xiao-Yue and Guo, Chu and Tian, Guojing and Wei, Shi-Jie and Sun, Xiaoming and Bao, Wan-Su and Long, Gui-Lu},
  year = {2023},
  month = apr,
  journal = {Science China Physics, Mechanics \& Astronomy},
  volume = {66},
  number = {5},
  pages = {250302},
  issn = {1869-1927},
  doi = {10.1007/s11433-022-2057-y},
  urldate = {2023-10-07},
  abstract = {Quantum computing is a game-changing technology for global academia, research centers and industries including computational science, mathematics, finance, pharmaceutical, materials science, chemistry and cryptography. Although it has seen a major boost in the last decade, we are still a long way from reaching the maturity of a full-fledged quantum computer. That said, we will be in the noisy-intermediate scale quantum (NISQ) era for a long time, working on dozens or even thousands of qubits quantum computing systems. An outstanding challenge, then, is to come up with an application that can reliably carry out a nontrivial task of interest on the near-term quantum devices with non-negligible quantum noise. To address this challenge, several near-term quantum computing techniques, including variational quantum algorithms, error mitigation, quantum circuit compilation and benchmarking protocols, have been proposed to characterize and mitigate errors, and to implement algorithms with a certain resistance to noise, so as to enhance the capabilities of near-term quantum devices and explore the boundaries of their ability to realize useful applications. Besides, the development of near-term quantum devices is inseparable from the efficient classical simulation, which plays a vital role in quantum algorithm design and verification, error-tolerant verification and other applications. This review will provide a thorough introduction of these near-term quantum computing techniques, report on their progress, and finally discuss the future prospect of these techniques, which we hope will motivate researchers to undertake additional studies in this field.},
  langid = {english},
  keywords = {benchmarking protocols,circuit compilation,classical simulation,error mitigation,noisy-intermediate scale quantum,quantum computing,variational quantum algorithms},
  file = {/home/jsimonrichard/Zotero/storage/PIFQMS68/Huang et al. - 2023 - Near-term quantum computing techniques Variationa.pdf}
}

@inproceedings{huDesignQuantumGraph2022,
  title = {On the {{Design}} of {{Quantum Graph Convolutional Neural Network}} in the {{NISQ-Era}} and {{Beyond}}},
  booktitle = {2022 {{IEEE}} 40th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Hu, Zhirui and Li, Jinyang and Pan, Zhenyu and Zhou, Shanglin and Yang, Lei and Ding, Caiwen and Khan, Omer and Geng, Tong and Jiang, Weiwen},
  year = {2022},
  month = oct,
  pages = {290--297},
  issn = {2576-6996},
  doi = {10.1109/ICCD56317.2022.00050},
  urldate = {2023-12-22},
  abstract = {The rapid growth in the size of Graph Convolutional Neural Networks (GCNs) encounters both computational- and memory-wall on classical computing platforms (e.g., CPU, GPU, FPGA, etc.). Quantum computing, on the other hand, provides extremely high parallelism for computation. Although quantum neural networks have been recently studied, the research on quantum graph neural networks is still in its infancy. The key challenge here is how to integrate both the graph topology information and the learning ability of GCNs into quantum circuits. In this work, we leverage the Givens rotations and its quantum implementation to encode graph information; in addition, we employ the widely used variational quantum circuit to bring the learnable parameters. On top of these, we present a full-quantum design of Graph Convolutional Neural Networks, namely "QuGCN", for semi-supervised learning on graph-structured data. Experiment results show our design is competitive with classical GCNs in terms of node classification accuracy on Cora sub-dataset. More importantly, we show the potential advantages that can be achieved by the proposed quantum GCN design when the number of features grows.},
  keywords = {Cora Dataset},
  file = {/home/jsimonrichard/Zotero/storage/XYGLKAHG/Hu et al. - 2022 - On the Design of Quantum Graph Convolutional Neura.pdf;/home/jsimonrichard/Zotero/storage/4HBAR73G/9978396.html}
}

@inproceedings{huDesignQuantumGraph2022a,
  title = {On the {{Design}} of {{Quantum Graph Convolutional Neural Network}} in the {{NISQ-Era}} and {{Beyond}}},
  booktitle = {2022 {{IEEE}} 40th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Hu, Zhirui and Li, Jinyang and Pan, Zhenyu and Zhou, Shanglin and Yang, Lei and Ding, Caiwen and Khan, Omer and Geng, Tong and Jiang, Weiwen},
  year = {2022},
  month = oct,
  pages = {290--297},
  issn = {2576-6996},
  doi = {10.1109/ICCD56317.2022.00050},
  urldate = {2023-12-19},
  abstract = {The rapid growth in the size of Graph Convolutional Neural Networks (GCNs) encounters both computational- and memory-wall on classical computing platforms (e.g., CPU, GPU, FPGA, etc.). Quantum computing, on the other hand, provides extremely high parallelism for computation. Although quantum neural networks have been recently studied, the research on quantum graph neural networks is still in its infancy. The key challenge here is how to integrate both the graph topology information and the learning ability of GCNs into quantum circuits. In this work, we leverage the Givens rotations and its quantum implementation to encode graph information; in addition, we employ the widely used variational quantum circuit to bring the learnable parameters. On top of these, we present a full-quantum design of Graph Convolutional Neural Networks, namely "QuGCN", for semi-supervised learning on graph-structured data. Experiment results show our design is competitive with classical GCNs in terms of node classification accuracy on Cora sub-dataset. More importantly, we show the potential advantages that can be achieved by the proposed quantum GCN design when the number of features grows.}
}

@inproceedings{huDesignQuantumGraph2022b,
  title = {On the {{Design}} of {{Quantum Graph Convolutional Neural Network}} in the {{NISQ-Era}} and {{Beyond}}},
  booktitle = {2022 {{IEEE}} 40th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Hu, Zhirui and Li, Jinyang and Pan, Zhenyu and Zhou, Shanglin and Yang, Lei and Ding, Caiwen and Khan, Omer and Geng, Tong and Jiang, Weiwen},
  year = {2022},
  month = oct,
  pages = {290--297},
  issn = {2576-6996},
  doi = {10.1109/ICCD56317.2022.00050},
  urldate = {2023-12-19},
  abstract = {The rapid growth in the size of Graph Convolutional Neural Networks (GCNs) encounters both computational- and memory-wall on classical computing platforms (e.g., CPU, GPU, FPGA, etc.). Quantum computing, on the other hand, provides extremely high parallelism for computation. Although quantum neural networks have been recently studied, the research on quantum graph neural networks is still in its infancy. The key challenge here is how to integrate both the graph topology information and the learning ability of GCNs into quantum circuits. In this work, we leverage the Givens rotations and its quantum implementation to encode graph information; in addition, we employ the widely used variational quantum circuit to bring the learnable parameters. On top of these, we present a full-quantum design of Graph Convolutional Neural Networks, namely "QuGCN", for semi-supervised learning on graph-structured data. Experiment results show our design is competitive with classical GCNs in terms of node classification accuracy on Cora sub-dataset. More importantly, we show the potential advantages that can be achieved by the proposed quantum GCN design when the number of features grows.},
  file = {/home/jsimonrichard/Zotero/storage/E7VLN64I/9978396.html}
}

@inproceedings{hutterEfficientApproachAssessing2014,
  title = {An {{Efficient Approach}} for {{Assessing Hyperparameter Importance}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Hutter, Frank and Hoos, Holger and {Leyton-Brown}, Kevin},
  year = {2014},
  month = jan,
  pages = {754--762},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-07-08},
  abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that---even in very high-dimensional cases---most performance variation is attributable to just a few hyperparameters.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/4ZX6PYRM/Hutter et al. - 2014 - An Efficient Approach for Assessing Hyperparameter.pdf}
}

@misc{IBMUnveils4002022,
  title = {{{IBM Unveils}} 400 {{Qubit-Plus Quantum Processor}} and {{Next-Generation IBM Quantum System Two}}},
  year = {2022},
  month = nov,
  publisher = {IBM},
  urldate = {2022-11-23},
  howpublished = {https://newsroom.ibm.com/2022-11-09-IBM-Unveils-400-Qubit-Plus-Quantum-Processor-and-Next-Generation-IBM-Quantum-System-Two}
}

@article{IDCForecastsWorldwide2021,
  title = {{{IDC Forecasts Worldwide Quantum Computing Market}} to {{Grow}} to \$8.6 {{Billion}} in 2027},
  year = {2021},
  month = nov,
  journal = {International Data Corporation},
  urldate = {2022-11-23}
}

@article{islamDeepLearningBased2021,
  title = {A Deep Learning Based Framework for the Registration of Three Dimensional Multi-Modal Medical Images of the Head},
  author = {Islam, Kh Tohidul and Wijewickrema, Sudanthi and O'Leary, Stephen},
  year = {2021},
  month = jan,
  journal = {Scientific Reports 2021 11:1},
  volume = {11},
  number = {1},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-81044-7},
  urldate = {2022-12-11},
  abstract = {Image registration is a fundamental task in image analysis in which the transform that moves the coordinate system of one image to another is calculated. Registration of multi-modal medical images has important implications for clinical diagnosis, treatment planning, and image-guided surgery as it provides the means of bringing together complimentary information obtained from different image modalities. However, since different image modalities have different properties due to their different acquisition methods, it remains a challenging task to find a fast and accurate match between multi-modal images. Furthermore, due to reasons such as ethical issues and need for human expert intervention, it is difficult to collect a large database of labelled multi-modal medical images. In addition, manual input is required to determine the fixed and moving images as input to registration algorithms. In this paper, we address these issues and introduce a registration framework that (1) creates synthetic data to augment existing datasets, (2) generates ground truth data to be used in the training and testing of algorithms, (3) registers (using a combination of deep learning and conventional machine learning methods) multi-modal images in an accurate and fast manner, and (4) automatically classifies the image modality so that the process of registration can be fully automated. We validate the performance of the proposed framework on CT and MRI images of the head obtained from a publicly available registration database.},
  isbn = {0123456789},
  pmid = {33479305},
  keywords = {Computer science,Software,Statistics},
  file = {/home/jsimonrichard/Zotero/storage/XVCBNE6F/full-text.pdf}
}

@misc{izadiOptimizationGraphNeural2020,
  title = {Optimization of {{Graph Neural Networks}} with {{Natural Gradient Descent}}},
  author = {Izadi, Mohammad Rasool and Fang, Yihao and Stevenson, Robert and Lin, Lizhen},
  year = {2020},
  month = aug,
  number = {arXiv:2008.09624},
  eprint = {2008.09624},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.09624},
  urldate = {2024-02-10},
  abstract = {In this work, we propose to employ information-geometric tools to optimize a graph neural network architecture such as the graph convolutional networks. More specifically, we develop optimization algorithms for the graph-based semi-supervised learning by employing the natural gradient information in the optimization process. This allows us to efficiently exploit the geometry of the underlying statistical model or parameter space for optimization and inference. To the best of our knowledge, this is the first work that has utilized the natural gradient for the optimization of graph neural networks that can be extended to other semi-supervised problems. Efficient computations algorithms are developed and extensive numerical studies are conducted to demonstrate the superior performance of our algorithms over existing algorithms such as ADAM and SGD.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Cora Dataset,Statistics - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/NFIBD9LV/Izadi et al. - 2020 - Optimization of Graph Neural Networks with Natural.pdf;/home/jsimonrichard/Zotero/storage/MD2DK46T/2008.html}
}

@article{j.QuantumAlgorithmImplementations2022,
  title = {Quantum {{Algorithm Implementations}} for {{Beginners}}},
  author = {J., Abhijith and Adedoyin, Adetokunbo and Ambrosiano, John and Anisimov, Petr and Casper, William and Chennupati, Gopinath and Coffrin, Carleton and Djidjev, Hristo and Gunter, David and Karra, Satish and Lemons, Nathan and Lin, Shizeng and Malyzhenkov, Alexander and Mascarenas, David and Mniszewski, Susan and Nadiga, Balu and O'Malley, Daniel and Oyen, Diane and Pakin, Scott and Prasad, Lakshman and Roberts, Randy and Romero, Phillip and Santhi, Nandakishore and Sinitsyn, Nikolai and Swart, Pieter J. and Wendelberger, James G. and Yoon, Boram and Zamora, Richard and Zhu, Wei and Eidenbenz, Stephan and B{\"a}rtschi, Andreas and Coles, Patrick J. and Vuffray, Marc and Lokhov, Andrey Y.},
  year = {2022},
  month = dec,
  journal = {ACM Transactions on Quantum Computing},
  volume = {3},
  number = {4},
  eprint = {1804.03719},
  primaryclass = {quant-ph},
  pages = {1--92},
  issn = {2643-6809, 2643-6817},
  doi = {10.1145/3517340},
  urldate = {2023-09-11},
  abstract = {As quantum computers become available to the general public, the need has arisen to train a cohort of quantum programmers, many of whom have been developing classical computer programs for most of their careers. While currently available quantum computers have less than 100 qubits, quantum computing hardware is widely expected to grow in terms of qubit count, quality, and connectivity. This review aims to explain the principles of quantum programming, which are quite different from classical programming, with straightforward algebra that makes understanding of the underlying fascinating quantum mechanical principles optional. We give an introduction to quantum computing algorithms and their implementation on real quantum hardware. We survey 20 different quantum algorithms, attempting to describe each in a succinct and self-contained fashion. We show how these algorithms can be implemented on IBM's quantum computer, and in each case, we discuss the results of the implementation with respect to differences between the simulator and the actual hardware runs. This article introduces computer scientists, physicists, and engineers to quantum algorithms and provides a blueprint for their implementations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/NXEXK5I3/J. et al. - 2022 - Quantum Algorithm Implementations for Beginners.pdf;/home/jsimonrichard/Zotero/storage/4AC8QNHY/1804.html}
}

@article{jadhavQuantumMachineLearning2023,
  title = {Quantum {{Machine Learning}}: {{Scope}} for Real-World Problems},
  shorttitle = {Quantum {{Machine Learning}}},
  author = {Jadhav, Abhishek and Rasool, Akhtar and Gyanchandani, Manasi},
  year = {2023},
  month = jan,
  journal = {Procedia Computer Science},
  series = {International {{Conference}} on {{Machine Learning}} and {{Data Engineering}}},
  volume = {218},
  pages = {2612--2625},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2023.01.235},
  urldate = {2024-05-04},
  abstract = {Quantum computing with its inherent parallelism provides a quantum advantage over classical computing. Its potential to offer breakthrough advances in various areas of science and engineering is foreseen. Machine learning is one of the key areas where the power of quantum computing can be utilized. Though many machine learning algorithms have been successfully developed to solve a variety of problems in the past decades, these algorithms take a long time to train. Also working on today's colossal datasets makes these algorithms computationally intensive. Quantum machine learning by utilizing the concepts of superposition and entanglement promises a solution to this problem. Quantum machine learning algorithms are in surface for the past few years and majority of the current research has dealt with the two machine learning problems namely classification and clustering. In this paper, a brief review of the recent techniques and algorithms of quantum machine learning and its scope in solving real world problems is studied.},
  keywords = {Quantum algorithms,Quantum machine learning,Quantum subroutines},
  file = {/home/jsimonrichard/Zotero/storage/LS6BSQEF/Jadhav et al. - 2023 - Quantum Machine Learning Scope for real-world pro.pdf;/home/jsimonrichard/Zotero/storage/VLNFE5VU/S1877050923002351.html}
}

@article{jiangCouldGraphNeural2021,
  title = {Could Graph Neural Networks Learn Better Molecular Representation for Drug Discovery? {{A}} Comparison Study of Descriptor-Based and Graph-Based Models},
  shorttitle = {Could Graph Neural Networks Learn Better Molecular Representation for Drug Discovery?},
  author = {Jiang, Dejun and Wu, Zhenxing and Hsieh, Chang-Yu and Chen, Guangyong and Liao, Ben and Wang, Zhe and Shen, Chao and Cao, Dongsheng and Wu, Jian and Hou, Tingjun},
  year = {2021},
  month = feb,
  journal = {Journal of Cheminformatics},
  volume = {13},
  number = {1},
  pages = {12},
  issn = {1758-2946},
  doi = {10.1186/s13321-020-00479-8},
  urldate = {2023-11-12},
  abstract = {Graph neural networks (GNN) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that GNN could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning (ML) algorithms, including four descriptor-based models (SVM, XGBoost, RF and DNN) and four graph-based models (GCN, GAT, MPNN and Attentive FP), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. SVM generally achieves the best predictions for the regression tasks. Both RF and XGBoost can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive FP and GCN, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, XGBoost and RF are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the SHAP method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening (VS) towards HIV and demonstrated that different ML algorithms offer diverse VS profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.},
  keywords = {ADME/T prediction,Deep learning,Ensemble learning,Extreme gradient boosting,Graph neural networks},
  file = {/home/jsimonrichard/Zotero/storage/S7E25QPQ/Jiang et al. - 2021 - Could graph neural networks learn better molecular.pdf;/home/jsimonrichard/Zotero/storage/LYK8R75R/s13321-020-00479-8.html}
}

@article{jonesImplementationQuantumAlgorithm1998,
  title = {Implementation of a Quantum Algorithm on a Nuclear Magnetic Resonance Quantum Computer},
  author = {Jones, Jonathan A and Mosca, Michele},
  year = {1998},
  journal = {The Journal of chemical physics},
  volume = {109},
  number = {5},
  pages = {1648--1653},
  publisher = {American Institute of Physics}
}

@article{jungRustBeltSecuringFoundations2017,
  title = {{{RustBelt}}: Securing the Foundations of the {{Rust}} Programming Language},
  shorttitle = {{{RustBelt}}},
  author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
  year = {2017},
  month = dec,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {POPL},
  pages = {66:1--66:34},
  doi = {10.1145/3158154},
  urldate = {2023-10-10},
  abstract = {Rust is a new systems programming language that promises to overcome the seemingly fundamental tradeoff between high-level safety guarantees and low-level control over resource management. Unfortunately, none of Rust's safety claims have been formally proven, and there is good reason to question whether they actually hold. Specifically, Rust employs a strong, ownership-based type system, but then extends the expressive power of this core type system through libraries that internally use unsafe features. In this paper, we give the first formal (and machine-checked) safety proof for a language representing a realistic subset of Rust. Our proof is extensible in the sense that, for each new Rust library that uses unsafe features, we can say what verification condition it must satisfy in order for it to be deemed a safe extension to the language. We have carried out this verification for some of the most important libraries that are used throughout the Rust ecosystem.},
  keywords = {concurrency,logical relations,Rust,separation logic,type systems},
  file = {/home/jsimonrichard/Zotero/storage/333ZDABF/Jung et al. - 2017 - RustBelt securing the foundations of the Rust pro.pdf}
}

@article{jungSafeSystemsProgramming2021,
  title = {Safe Systems Programming in {{Rust}}},
  author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
  year = {2021},
  month = mar,
  journal = {Communications of the ACM},
  volume = {64},
  number = {4},
  pages = {144--152},
  issn = {0001-0782},
  doi = {10.1145/3418295},
  urldate = {2023-10-10},
  abstract = {The promise and the challenges of the first industry-supported language to master the trade-off between safety and control.},
  file = {/home/jsimonrichard/Zotero/storage/W4NPYBY8/Jung et al. - 2021 - Safe systems programming in Rust.pdf}
}

@article{kaoExploringAdvantagesQuantum2023,
  title = {Exploring the {{Advantages}} of {{Quantum Generative Adversarial Networks}} in {{Generative Chemistry}}},
  author = {Kao, Po-Yu and Yang, Ya-Chu and Chiang, Wei-Yin and Hsiao, Jen-Yueh and Cao, Yudong and Aliper, Alex and Ren, Feng and {Aspuru-Guzik}, Al{\'a}n and Zhavoronkov, Alex and Hsieh, Min-Hsiu and Lin, Yen-Chu},
  year = {2023},
  month = jun,
  journal = {Journal of Chemical Information and Modeling},
  volume = {63},
  number = {11},
  pages = {3307--3318},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.3c00562},
  urldate = {2023-10-07},
  abstract = {De novo drug design with desired biological activities is crucial for developing novel therapeutics for patients. The drug development process is time- and resource-consuming, and it has a low probability of success. Recent advances in machine learning and deep learning technology have reduced the time and cost of the discovery process and therefore, improved pharmaceutical research and development. In this paper, we explore the combination of two rapidly developing fields with lead candidate discovery in the drug development process. First, artificial intelligence has already been demonstrated to successfully accelerate conventional drug design approaches. Second, quantum computing has demonstrated promising potential in different applications, such as quantum chemistry, combinatorial optimizations, and machine learning. This article explores hybrid quantum-classical generative adversarial networks (GAN) for small molecule discovery. We substituted each element of GAN with a variational quantum circuit (VQC) and demonstrated the quantum advantages in the small drug discovery. Utilizing a VQC in the noise generator of a GAN to generate small molecules achieves better physicochemical properties and performance in the goal-directed benchmark than the classical counterpart. Moreover, we demonstrate the potential of a VQC with only tens of learnable parameters in the generator of GAN to generate small molecules. We also demonstrate the quantum advantage of a VQC in the discriminator of GAN. In this hybrid model, the number of learnable parameters is significantly less than the classical ones, and it can still generate valid molecules. The hybrid model with only tens of training parameters in the quantum discriminator outperforms the MLP-based one in terms of both generated molecule properties and the achieved KL divergence. However, the hybrid quantum-classical GANs still face challenges in generating unique and valid molecules compared to their classical counterparts.},
  file = {/home/jsimonrichard/Zotero/storage/HMXG57F2/Kao et al. - 2023 - Exploring the Advantages of Quantum Generative Adv.pdf;/home/jsimonrichard/Zotero/storage/3TJPCGJ9/acs.jcim.html}
}

@article{keeRSADeadWe2021,
  title = {{{RSA Is Dead}} --- {{We Just Haven}}'t {{Accepted It Yet}}},
  author = {Kee, Lila},
  year = {2021},
  month = may,
  journal = {Forbes},
  urldate = {2022-12-03}
}

@book{kernighanProgrammingLanguage,
  title = {The {{C Programming Language}}},
  author = {Kernighan, Brian W. and Ritchie, Dennis M.},
  edition = {2},
  file = {/home/jsimonrichard/Zotero/storage/NUTWVNW7/The C Programming Language.pdf}
}

@article{khoshamanQuantumVariationalAutoencoder2018,
  title = {Quantum Variational Autoencoder},
  author = {Khoshaman, Amir and Vinci, Walter and Denis, Brandon and Andriyash, Evgeny and Sadeghi, Hossein and Amin, Mohammad H.},
  year = {2018},
  month = sep,
  journal = {Quantum Science and Technology},
  volume = {4},
  number = {1},
  pages = {014001},
  publisher = {IOP Publishing},
  issn = {2058-9565},
  doi = {10.1088/2058-9565/aada1f},
  urldate = {2023-09-30},
  abstract = {Variational autoencoders (VAEs) are powerful generative models with the salient ability to perform inference. Here, we introduce a quantum variational autoencoder (QVAE): a VAE whose latent generative process is implemented as a quantum Boltzmann machine (QBM). We show that our model can be trained end-to-end by maximizing a well-defined loss-function: a `quantum' lower-bound to a variational approximation of the log-likelihood. We use quantum Monte Carlo (QMC) simulations to train and evaluate the performance of QVAEs. To achieve the best performance, we first create a VAE platform with discrete latent space generated by a restricted Boltzmann machine. Our model achieves state-of-the-art performance on the MNIST dataset when compared against similar approaches that only involve discrete variables in the generative process. We consider QVAEs with a smaller number of latent units to be able to perform QMC simulations, which are computationally expensive. We show that QVAEs can be trained effectively in regimes where quantum effects are relevant despite training via the quantum bound. Our findings open the way to the use of quantum computers to train QVAEs to achieve competitive performance for generative models. Placing a QBM in the latent space of a VAE leverages the full potential of current and next-generation quantum computers as sampling devices.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/PDUISXJR/Khoshaman et al. - 2018 - Quantum variational autoencoder.pdf}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2024-05-06},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/UEUW8PML/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/jsimonrichard/Zotero/storage/ISB2R2UT/1412.html}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.02907},
  urldate = {2024-01-06},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/9XJ5WW3R/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/home/jsimonrichard/Zotero/storage/54PV2DD8/1609.html}
}

@misc{knillEfficientLinearOptics2000,
  title = {Efficient {{Linear Optics Quantum Computation}}},
  author = {Knill, E. and Laflamme, R. and Milburn, G.},
  year = {2000},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.QUANT-PH/0006088},
  keywords = {FOS: Physical sciences,Quantum Physics (quant-ph)}
}

@inproceedings{kochharLargeScaleStudy2016,
  title = {A {{Large Scale Study}} of {{Multiple Programming Languages}} and {{Code Quality}}},
  booktitle = {2016 {{IEEE}} 23rd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  author = {Kochhar, Pavneet Singh and Wijedasa, Dinusha and Lo, David},
  year = {2016},
  month = mar,
  volume = {1},
  pages = {563--573},
  doi = {10.1109/SANER.2016.112},
  urldate = {2023-10-10},
  abstract = {Nowadays, most software use multiple programming languages to implement certain functionalities based on the strengths and weaknesses of different languages. Researchers in the past have studied the impact of independent programming languages on software quality, however, there has been little or no research on the impact of multiple languages on the quality of software. Does the use of multiple languages cause more bugs? Are certain languages when used with other languages make software more bug prone? What are the relationships between multi-language usage and various bug categories? In this study, we perform a large scale empirical investigation to provide some answers to these questions. We gather a large dataset consisting of popular projects from GitHub (628 projects, 85 million SLOC, 134 thousand authors, 3 million commits, in 17 languages) to understand the impact of using multiple languages on software quality. We build multiple regression models to study the effects of using different languages on the number of bug fixing commits while controlling for factors such as project age, project size, team size, and the number of commits. Our results show that in general implementing a project with more languages has a significant effect on project quality, as it increases defect proneness. Moreover, we find specific languages that are statistically significantly more defect prone when they are used in a multi-language setting. These include popular languages like C++, Objective-C, and Java. Furthermore, we note that the use of more languages significantly increases bug proneness across all bug categories. The effect is strongest for memory, concurrency, and algorithm bugs.},
  file = {/home/jsimonrichard/Zotero/storage/XZGDWQZU/7476675.html}
}

@inproceedings{koptaFastEffectiveBVH2012,
  title = {Fast, Effective {{BVH}} Updates for Animated Scenes},
  booktitle = {Proceedings - {{I3D}} 2012: {{ACM SIGGRAPH Symposium}} on {{Interactive 3D Graphics}} and {{Games}}},
  author = {Kopta, Daniel and Ize, Thiago and Spjut, Josef and Brunvand, Erik and Davis, Al and Kensler, Andrew},
  year = {2012},
  doi = {10.1145/2159616.2159649},
  abstract = {Bounding volume hierarchies (BVHs) are a popular acceleration structure choice for animated scenes rendered with ray tracing. This is due to the relative simplicity of refitting bounding volumes around moving geometry. However, the quality of such a refitted tree can degrade rapidly if objects in the scene deform or rearrange significantly as the animation progresses, resulting in dramatic increases in rendering times and a commensurate reduction in the frame rate. The BVH could be rebuilt on every frame, but this could take significant time. We present a method to efficiently extend refitting for animated scenes with tree rotations, a technique previously proposed for off-line improvement of BVH quality for static scenes. Tree rotations are local restructuring operations which can mitigate the effects that moving primitives have on BVH quality by rearranging nodes in the tree during each refit rather than triggering a full rebuild. The result is a fast, lightweight, incremental update algorithm that requires negligible memory, has minor update times, parallelizes easily, avoids significant degradation in tree quality or the need for rebuilding, and maintains fast rendering times. We show that our method approaches or exceeds the frame rates of other techniques and is consistently among the best options regardless of the animated scene. {\copyright} 2012 ACM.},
  file = {/home/jsimonrichard/Zotero/storage/U9EE2C7S/Kopta et al. - 2012 - Fast, effective BVH updates for animated scenes.pdf}
}

@article{lanyonExperimentalQuantumComputing2008,
  title = {Experimental Quantum Computing without Entanglement},
  author = {Lanyon, B. P. and Barbieri, M. and Almeida, M. P. and White, A. G.},
  year = {2008},
  journal = {Physical Review Letters},
  volume = {101},
  number = {20},
  issn = {00319007},
  doi = {10.1103/PhysRevLett.101.200501},
  abstract = {Deterministic quantum computation with one pure qubit (DQC1) is an efficient model of computation that uses highly mixed states. Unlike pure-state models, its power is not derived from the generation of a large amount of entanglement. Instead it has been proposed that other nonclassical correlations are responsible for the computational speedup, and that these can be captured by the quantum discord. In this Letter we implement DQC1 in an all-optical architecture, and experimentally observe the generated correlations. We find no entanglement, but large amounts of quantum discord-except in three cases where an efficient classical simulation is always possible. Our results show that even fully separable, highly mixed, states can contain intrinsically quantum mechanical correlations and that these could offer a valuable resource for quantum information technologies. {\copyright} 2008 The American Physical Society.}
}

@article{leydenDownsSyndromeScreening2001,
  title = {Down's {{Syndrome}} Screening Failures Linked to {{Y2K}} Bug},
  author = {Leyden, John},
  year = {2001},
  month = sep,
  journal = {The Register},
  urldate = {2022-12-05}
}

@misc{liDrugDiscoveryApproaches2021,
  title = {Drug {{Discovery Approaches}} Using {{Quantum Machine Learning}}},
  author = {Li, Junde and Alam, Mahabubul and Sha, Congzhou M. and Wang, Jian and Dokholyan, Nikolay V. and Ghosh, Swaroop},
  year = {2021},
  month = apr,
  journal = {arXiv.org},
  urldate = {2023-10-08},
  abstract = {Traditional drug discovery pipeline takes several years and cost billions of dollars. Deep generative and predictive models are widely adopted to assist in drug development. Classical machines cannot efficiently produce atypical patterns of quantum computers which might improve the training quality of learning tasks. We propose a suite of quantum machine learning techniques e.g., generative adversarial network (GAN), convolutional neural network (CNN) and variational auto-encoder (VAE) to generate small drug molecules, classify binding pockets in proteins, and generate large drug molecules, respectively.},
  howpublished = {https://arxiv.org/abs/2104.00746v1},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/J7GI3H9R/Li et al. - 2021 - Drug Discovery Approaches using Quantum Machine Le.pdf}
}

@misc{liImprovingSemanticSegmentation2020,
  title = {Improving {{Semantic Segmentation}} via {{Decoupled Body}} and {{Edge Supervision}}},
  author = {Li, Xiangtai and Li, Xia and Zhang, Li and Cheng, Guangliang and Shi, Jianping and Lin, Zhouchen and Tan, Shaohua and Tong, Yunhai},
  year = {2020},
  month = aug,
  number = {arXiv:2007.10035},
  eprint = {2007.10035},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.10035},
  urldate = {2022-07-19},
  abstract = {Existing semantic segmentation approaches either aim to improve the object's inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires {\textbackslash}textit\{explicitly\} modeling the object {\textbackslash}textit\{body\} and {\textbackslash}textit\{edge\}, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including {\textbackslash}textit\{Cityscapes\}, {\textbackslash}textit\{CamVid\}, {\textbackslash}textit\{KIITI\} and {\textbackslash}textit\{BDD\} show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU {\textbackslash}\% on Cityscape with only fine-annotated data. Code and models are made available to foster any further research ({\textbackslash}url\{https://github.com/lxtGH/DecoupleSegNets\}).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jsimonrichard/Zotero/storage/6T34QPJT/Li et al. - 2020 - Improving Semantic Segmentation via Decoupled Body.pdf;/home/jsimonrichard/Zotero/storage/9B9VWQMK/Li et al. - 2020 - Improving Semantic Segmentation via Decoupled Body.html}
}

@misc{liQuantumGenerativeModels2021,
  title = {Quantum {{Generative Models}} for {{Small Molecule Drug Discovery}}},
  author = {Li, Junde and Topaloglu, Rasit and Ghosh, Swaroop},
  year = {2021},
  month = jan,
  journal = {arXiv.org},
  urldate = {2023-10-08},
  abstract = {Existing drug discovery pipelines take 5-10 years and cost billions of dollars. Computational approaches aim to sample from regions of the whole molecular and solid-state compounds called chemical space which could be on the order of 1060 . Deep generative models can model the underlying probability distribution of both the physical structures and property of drugs and relate them nonlinearly. By exploiting patterns in massive datasets, these models can distill salient features that characterize the molecules. Generative Adversarial Networks (GANs) discover drug candidates by generating molecular structures that obey chemical and physical properties and show affinity towards binding with the receptor for a target disease. However, classical GANs cannot explore certain regions of the chemical space and suffer from curse-of-dimensionality. A full quantum GAN may require more than 90 qubits even to generate QM9-like small molecules. We propose a qubit-efficient quantum GAN with a hybrid generator (QGAN-HG) to learn richer representation of molecules via searching exponentially large chemical space with few qubits more efficiently than classical GAN. The QGANHG model is composed of a hybrid quantum generator that supports various number of qubits and quantum circuit layers, and, a classical discriminator. QGAN-HG with only 14.93\% retained parameters can learn molecular distribution as efficiently as classical counterpart. The QGAN-HG variation with patched circuits considerably accelerates our standard QGANHG training process and avoids potential gradient vanishing issue of deep neural networks. Code is available on GitHub https://github.com/jundeli/quantum-gan.},
  howpublished = {https://arxiv.org/abs/2101.03438v1},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/S97RWF7M/Li et al. - 2021 - Quantum Generative Models for Small Molecule Drug .pdf}
}

@article{liQuantumSecureDirect2007,
  title = {Quantum Secure Direct Communication with Quantum Encryption Based on Pure Entangled States},
  author = {Li, Xi Han and Li, Chun Yan and Deng, Fu Guo and Zhou, Ping and Liang, Yu Jie and Zhou, Hong Yu},
  year = {2007},
  journal = {Chinese Physics},
  volume = {16},
  number = {8},
  issn = {10091963},
  doi = {10.1088/1009-1963/16/8/001},
  abstract = {This paper presents a scheme for quantum secure direct communication with quantum encryption. The two authorized users use repeatedly a sequence of the pure entangled pairs (quantum key) shared for encrypting and decrypting the secret message carried by the travelling photons directly. For checking eavesdropping, the two parties perform the single-photon measurements on some decoy particles before each round. This scheme has the advantage that the pure entangled quantum signal source is feasible at present and any eavesdropper cannot steal the message. {\copyright} 2007 Chin. Phys. Soc. and IOP Publishing Ltd.}
}

@misc{lloydQuantumAlgorithmsSupervised2013,
  title = {Quantum Algorithms for Supervised and Unsupervised Machine Learning},
  author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
  year = {2013},
  month = nov,
  number = {arXiv:1307.0411},
  eprint = {1307.0411},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1307.0411},
  urldate = {2023-09-11},
  abstract = {Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/JMPVLPBP/Lloyd et al. - 2013 - Quantum algorithms for supervised and unsupervised.pdf;/home/jsimonrichard/Zotero/storage/DUMHRR36/1307.html}
}

@article{macquarrieEmergingCommercialLandscape2020,
  title = {The Emerging Commercial Landscape of Quantum Computing},
  author = {MacQuarrie, Evan R. and Simon, Christoph and Simmons, Stephanie and Maine, Elicia},
  year = {2020},
  month = oct,
  journal = {Nature Reviews Physics},
  volume = {2},
  number = {11},
  pages = {596--598},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/s42254-020-00247-5}
}

@article{maggioriniSmashDistributedGame,
  title = {Smash: {{A}} Distributed Game Engine Architecture},
  author = {Maggiorini, Dario and Ripamonti, Laura Anna and Zanon, Eraldo and Palazzi, Claudio Enrico and Bujari, Armir},
  journal = {ieeexplore.ieee.org},
  urldate = {2022-12-15},
  abstract = {In these last few years we are witnessing a change in the way video games are implemented. Starting from an early age, where a single developer was sometimes in charge of the whole creative process, we have moved now toward extremely large groups with a multi-layered organisation. This increasing complexity of team organisation, together with a tremendous growth of projects size, calls for the adoptions of development approaches leveraging on scalability and distributed computing environments. Unfortunately, today's development and execution environments for games (usually called game engines) are suffering from a number of architectural constraints. As a result, we strongly believe current engines will not be able to provide the flexibility and scalability required by game developers of the next generation. To overcome the above limitations, in this paper we propose SMASH (Stackless Microkernel Architecture for SHared environments): an architecture where a game engine is decomposed in several dynamic and independent software modules interacting via a microkernel-like message bus. Game modules can just be inserted, debugged, and removed from a running engine once its internal messaging protocol is clearly defined. Moreover, following this approach, game modules can be dynamically dislocated on multiple machines to obtain a truly distributed, scalable, and fault-resilient system where adaptation can be achieved mostly without downtime.},
  file = {/home/jsimonrichard/Zotero/storage/SG8D2SNI/full-text.pdf}
}

@article{marxApproachingFivebitNMR2000,
  title = {Approaching Five-Bit {{NMR}} Quantum Computing},
  author = {Marx, R. and Fahmy, A. F. and Myers, John M. and Bermel, W. and Glaser, S. J.},
  year = {2000},
  month = jun,
  journal = {Phys. Rev. A},
  volume = {62},
  number = {1},
  pages = {012310},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.62.012310}
}

@inproceedings{mernyeiEquivariantQuantumGraph2022,
  title = {Equivariant {{Quantum Graph Circuits}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Mernyei, Peter and Meichanetzidis, Konstantinos and Ceylan, Ismail Ilkan},
  year = {2022},
  month = jun,
  pages = {15401--15420},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-02-19},
  abstract = {We investigate quantum circuits for graph representation learning, and propose equivariant quantum graph circuits (EQGCs), as a class of parameterized quantum circuits with strong relational inductive bias for learning over graph-structured data. Conceptually, EQGCs serve as a unifying framework for quantum graph representation learning, allowing us to define several interesting subclasses which subsume existing proposals. In terms of the representation power, we prove that the studied subclasses of EQGCs are universal approximators for functions over the bounded graph domain. This theoretical perspective on quantum graph machine learning methods opens many directions for further work, and could lead to models with capabilities beyond those of classical approaches. We empirically verify the expressive power of EQGCs through a dedicated experiment on synthetic data, and additionally observe that the performance of EQGCs scales well with the depth of the model and does not suffer from barren plateu issues.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/U66LTKF5/Mernyei et al. - 2022 - Equivariant Quantum Graph Circuits.pdf}
}

@article{millerEssaysComputingScience1991,
  title = {Essays in Computing Science},
  author = {Miller, C},
  year = {1991},
  month = mar,
  journal = {Information and Software Technology},
  volume = {33},
  number = {2},
  pages = {168},
  issn = {09505849},
  doi = {10.1016/0950-5849(91)90073-K},
  urldate = {2023-10-10},
  abstract = {Semantic Scholar extracted view of "Essays in computing science: C A R Hoare and C B Jones Prentice Hall (1988) 411 pp {\pounds}32.95 hardback" by C. Miller},
  langid = {english}
}

@article{montanaroQuantumAlgorithmsOverview2016,
  title = {Quantum Algorithms: An Overview},
  shorttitle = {Quantum Algorithms},
  author = {Montanaro, Ashley},
  year = {2016},
  month = jan,
  journal = {npj Quantum Information},
  volume = {2},
  number = {1},
  pages = {1--8},
  publisher = {Nature Publishing Group},
  issn = {2056-6387},
  doi = {10.1038/npjqi.2015.23},
  urldate = {2023-09-11},
  abstract = {Quantum computers are designed to outperform standard computers by running quantum algorithms. Areas in which quantum algorithms can be applied include cryptography, search and optimisation, simulation of quantum systems and solving large systems of linear equations. Here we briefly survey some known quantum algorithms, with an emphasis on a broad overview of their applications rather than their technical details. We include a discussion of recent developments and near-term applications of quantum algorithms.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Computer science,Quantum information},
  file = {/home/jsimonrichard/Zotero/storage/G9CV85FW/Montanaro - 2016 - Quantum algorithms an overview.pdf}
}

@misc{morrisTUDatasetCollectionBenchmark2020,
  title = {{{TUDataset}}: {{A}} Collection of Benchmark Datasets for Learning with Graphs},
  shorttitle = {{{TUDataset}}},
  author = {Morris, Christopher and Kriege, Nils M. and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
  year = {2020},
  month = jul,
  number = {arXiv:2007.08663},
  eprint = {2007.08663},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.08663},
  urldate = {2024-05-04},
  abstract = {Recently, there has been an increasing interest in (supervised) learning with graph data, especially using graph neural networks. However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging, consequently hindering advancements in this area. To address this, we introduce the TUDataset for graph classification and regression. The collection consists of over 120 datasets of varying sizes from a wide range of applications. We provide Python-based data loaders, kernel and graph neural network baseline implementations, and evaluation tools. Here, we give an overview of the datasets, standardized evaluation procedures, and provide baseline experiments. All datasets are available at www.graphlearning.io. The experiments are fully reproducible from the code available at www.github.com/chrsmrrs/tudataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{morrisTUDatasetCollectionBenchmark2020a,
  title = {{{TUDataset}}: {{A}} Collection of Benchmark Datasets for Learning with Graphs},
  booktitle = {{{ICML}} 2020 Workshop on Graph Representation Learning and beyond ({{GRL}}+ 2020)},
  author = {Morris, Christopher and Kriege, Nils M. and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
  year = {2020},
  file = {/home/jsimonrichard/Zotero/storage/UVTDWJZY/Morris et al. - 2020 - TUDataset A collection of benchmark datasets for .pdf;/home/jsimonrichard/Zotero/storage/7AEVB422/2007.html}
}

@misc{morrisWeisfeilerLemanGo2021,
  title = {Weisfeiler and {{Leman Go Neural}}: {{Higher-order Graph Neural Networks}}},
  shorttitle = {Weisfeiler and {{Leman Go Neural}}},
  author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  year = {2021},
  month = nov,
  number = {arXiv:1810.02244},
  eprint = {1810.02244},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.02244},
  urldate = {2024-04-26},
  abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically -- showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the \$1\$-dimensional Weisfeiler-Leman graph isomorphism heuristic (\$1\$-WL). We show that GNNs have the same expressiveness as the \$1\$-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called \$k\$-dimensional GNNs (\$k\$-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/8R3LINGK/Morris et al. - 2021 - Weisfeiler and Leman Go Neural Higher-order Graph.pdf;/home/jsimonrichard/Zotero/storage/ZIUBKHR7/1810.html}
}

@article{moscaCybersecurityEraQuantum2018,
  title = {Cybersecurity in an Era with Quantum Computers: {{Will}} We Be Ready?},
  author = {Mosca, Michele},
  year = {2018},
  journal = {IEEE Security and Privacy},
  volume = {16},
  number = {5},
  issn = {15584046},
  doi = {10.1109/MSP.2018.3761723},
  abstract = {Organizations must understand their specific risks and plan for their systems to be resilient to quantum attacks. Assessment is based on three quantities: the security shelf life of the information assets, the migration time to systems designed to resist quantum attacks, and the time remaining before quantum computers break the security.}
}

@misc{moscaQuantumAlgorithms2008,
  title = {Quantum {{Algorithms}}},
  author = {Mosca, Michele},
  year = {2008},
  month = aug,
  number = {arXiv:0808.0369},
  eprint = {0808.0369},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.0808.0369},
  urldate = {2023-09-11},
  abstract = {This article surveys the state of the art in quantum computer algorithms, including both black-box and non-black-box results. It is infeasible to detail all the known quantum algorithms, so a representative sample is given. This includes a summary of the early quantum algorithms, a description of the Abelian Hidden Subgroup algorithms (including Shor's factoring and discrete logarithm algorithms), quantum searching and amplitude amplification, quantum algorithms for simulating quantum mechanical systems, several non-trivial generalizations of the Abelian Hidden Subgroup Problem (and related techniques), the quantum walk paradigm for quantum algorithms, the paradigm of adiabatic algorithms, a family of ``topological'' algorithms, and algorithms for quantum tasks which cannot be done by a classical computer, followed by a discussion.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/LT8ZQ2QC/Mosca - 2008 - Quantum Algorithms.pdf;/home/jsimonrichard/Zotero/storage/3GTRJ8UV/0808.html}
}

@book{munkresTopology2023,
  title = {Topology},
  author = {Munkres, James},
  year = {2023},
  publisher = {Pearson},
  isbn = {0-13-181629-2},
  langid = {english}
}

@article{nakamuraCoherentControlMacroscopic1999,
  title = {Coherent Control of Macroscopic Quantum States in a Single-{{Cooper-pair}} Box},
  author = {Nakamura, Y. and Pashkin, {\relax Yu}. A. and Tsai, J. S.},
  year = {1999},
  month = apr,
  journal = {Nature},
  volume = {398},
  number = {6730},
  pages = {786--788},
  issn = {1476-4687},
  doi = {10.1038/19718}
}

@article{NISTAnnouncesFirst2022,
  title = {{{NIST Announces First Four Quantum-Resistant Cryptographic Algorithms}}},
  year = {2022},
  month = jul,
  journal = {National Institute of Standards and Technology},
  urldate = {2022-11-25}
}

@article{NISTAsksPublic2016,
  title = {{{NIST Asks Public}} to {{Help Future-Proof Electronic Informatiom}}},
  year = {2016},
  month = dec,
  journal = {National Institute of Standards and Technology},
  urldate = {2022-11-25}
}

@misc{odonnellLectureNotes15859BB2015,
  title = {Lecture {{Notes}} for 15-{{859BB}}: {{Quantum Computation}} and {{Information}}},
  author = {O'Donnell, Ryan},
  year = {2015},
  publisher = {Carnegie Mellon University},
  urldate = {2022-11-26},
  howpublished = {https://www.cs.cmu.edu/{\textasciitilde}odonnell/quantum15/}
}

@misc{OptunaHyperparameterOptimization,
  title = {Optuna - {{A}} Hyperparameter Optimization Framework},
  journal = {Optuna},
  urldate = {2024-04-27},
  abstract = {Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API.},
  howpublished = {https://optuna.org/},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/DFTPKLG3/optuna.org.html}
}

@article{orusQuantumComputingFinance2019,
  title = {Quantum Computing for Finance: {{Overview}} and Prospects},
  author = {Or{\'u}s, Rom{\'a}n and Mugel, Samuel and Lizaso, Enrique},
  year = {2019},
  journal = {Reviews in Physics},
  volume = {4},
  pages = {100028},
  issn = {2405-4283},
  doi = {10.1016/j.revip.2019.100028},
  abstract = {We discuss how quantum computation can be applied to financial problems, providing an overview of current approaches and potential prospects. We review quantum optimization algorithms, and expose how quantum annealers can be used to optimize portfolios, find arbitrage opportunities, and perform credit scoring. We also discuss deep-learning in finance, and suggestions to improve these methods through quantum machine learning. Finally, we consider quantum amplitude estimation, and how it can result in a quantum speed-up for Monte Carlo sampling. This has direct applications to many current financial methods, including pricing of derivatives and risk analysis. Perspectives are also discussed.},
  file = {/home/jsimonrichard/Zotero/storage/CH23XARV/Orús et al. - 2019 - Quantum computing for finance Overview and prospe.pdf}
}

@article{osullivanRealtimeCollisionDetection1999,
  title = {Real-Time Collision Detection and Response Using Sphere-Trees},
  author = {O'SULLIVAN, C and Dingliana, J},
  year = {1999},
  urldate = {2022-12-15},
  abstract = {In this paper we address the problem of collision detection and response in real-time animation systems. We describe an approach, which approximates objects using sphere-trees, and uses an interruptible detection algorithm to approximately test for collisions between them, trading accuracy for speed. A model of human visual perception of collisions is used to decide which collisions deserve more processing time. Collision processing is then scheduled to minimise the perceived inaccuracy within the time available. In response to such approximate collisions, a new adaptive collision response algorithm is presented, which also uses sphere-trees to approximate the appropriate response for colliding objects.},
  file = {/home/jsimonrichard/Zotero/storage/I73N393H/full-text.pdf}
}

@article{OverviewQuantumInitiatives2021,
  title = {Overview on Quantum Initiatives Worldwide -- Update Mid 2021},
  year = {2021},
  month = jul,
  journal = {Qureca},
  urldate = {2022-11-23}
}

@article{padavic-callaghanIBMUnveilsWorlds2022,
  title = {{{IBM}} Unveils World's Largest Quantum Computer at 433 Qubits},
  author = {{Padavic-Callaghan}, Karmela},
  year = {2022},
  month = nov,
  journal = {New Scientist},
  urldate = {2022-11-22}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning Library},
  shorttitle = {{{PyTorch}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {721},
  pages = {8026--8037},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-05-06},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  file = {/home/jsimonrichard/Zotero/storage/8LGHD6YR/Paszke et al. - 2019 - PyTorch an imperative style, high-performance dee.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  urldate = {2024-05-06},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {/home/jsimonrichard/Zotero/storage/3BSNZVBY/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@article{perez-salinasOneQubitUniversal2021,
  title = {One Qubit as a {{Universal Approximant}}},
  author = {{P{\'e}rez-Salinas}, Adri{\'a}n and {L{\'o}pez-N{\'u}{\~n}ez}, David and {Garc{\'i}a-S{\'a}ez}, Artur and {Forn-D{\'i}az}, P. and Latorre, Jos{\'e} I.},
  year = {2021},
  month = jul,
  journal = {Physical Review A},
  volume = {104},
  number = {1},
  eprint = {2102.04032},
  primaryclass = {quant-ph},
  pages = {012405},
  issn = {2469-9926, 2469-9934},
  doi = {10.1103/PhysRevA.104.012405},
  urldate = {2023-09-30},
  abstract = {A single-qubit circuit can approximate any bounded complex function stored in the degrees of freedom defining its quantum gates. The single-qubit approximant presented in this work is operated through a series of gates that take as their parameterization the independent variable of the target function and an additional set of adjustable parameters. The independent variable is re-uploaded in every gate while the parameters are optimized for each target function. The output state of this quantum circuit becomes more accurate as the number of re-uploadings of the independent variable increases, i. e., as more layers of gates parameterized with the independent variable are applied. In this work, we provide two different proofs of this claim related to both the Fourier series and the Universal Approximation Theorem for Neural Networks, and we benchmark both methods against their classical counterparts. We further implement a single-qubit approximant in a real superconducting qubit device, demonstrating how the ability to describe a set of functions improves with the depth of the quantum circuit. This work shows the robustness of the re-uploading technique on Quantum Machine Learning.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/APL7QLXC/Pérez-Salinas et al. - 2021 - One qubit as a Universal Approximant.pdf;/home/jsimonrichard/Zotero/storage/WII8BETD/2102.html}
}

@misc{PostQuantumAlgorithms,
  title = {Post {{Quantum Algorithms}}: {{Round}} 1 {{Submissions}}},
  publisher = {{National Institute of Standards and Technology}},
  urldate = {2022-11-25},
  howpublished = {https://csrc.nist.gov/Projects/post-quantum-cryptography/post-quantum-cryptography-standardization/round-1-submissions}
}

@misc{ProgrammingLanguagesLearning,
  title = {Programming {{Languages}} and {{Learning}} {\textbar} {{DO-IT}}},
  urldate = {2023-10-10},
  howpublished = {https://www.washington.edu/doit/programming-languages-and-learning},
  file = {/home/jsimonrichard/Zotero/storage/2NZCZFH7/programming-languages-and-learning.html}
}

@article{pyrkovQuantumComputingTerm2023,
  title = {Quantum Computing for Near-Term Applications in Generative Chemistry and Drug Discovery},
  author = {Pyrkov, Alexey and Aliper, Alex and Bezrukov, Dmitry and Lin, Yen-Chu and Polykovskiy, Daniil and Kamya, Petrina and Ren, Feng and Zhavoronkov, Alex},
  year = {2023},
  month = aug,
  journal = {Drug Discovery Today},
  volume = {28},
  number = {8},
  pages = {103675},
  issn = {1359-6446},
  doi = {10.1016/j.drudis.2023.103675},
  urldate = {2023-10-07},
  abstract = {In recent years, drug discovery and life sciences have been revolutionized with machine learning and artificial intelligence (AI) methods. Quantum computing is touted to be the next most significant leap in technology; one of the main early practical applications for quantum computing solutions is predicted to be in quantum chemistry simulations. Here, we review the near-term applications of quantum computing and their advantages for generative chemistry and highlight the challenges that can be addressed with noisy intermediate-scale quantum (NISQ) devices. We also discuss the possible integration of generative systems running on quantum computers into established generative AI platforms.},
  keywords = {artificial intelligence for drug discovery,generation of small molecules,generative chemistry,noisy intermediate-scale quantum (NISQ) devices,quantum chemistry,Quantum computing,quantum machine learning},
  file = {/home/jsimonrichard/Zotero/storage/LVG2I5VU/Pyrkov et al. - 2023 - Quantum computing for near-term applications in ge.pdf;/home/jsimonrichard/Zotero/storage/TJI58R7K/S1359644623001915.html}
}

@misc{Quantum_Graph_Recurrent_Neural_NetworkQgrnnipynbMain,
  title = {Quantum\_{{Graph}}\_{{Recurrent}}\_{{Neural}}\_{{Network}}/Qgrnn.Ipynb at Main {$\cdot$} Sajen-k/{{Quantum}}\_{{Graph}}\_{{Recurrent}}\_{{Neural}}\_{{Network}}},
  urldate = {2023-12-22},
  howpublished = {https://github.com/sajen-k/Quantum\_Graph\_Recurrent\_Neural\_Network/blob/main/qgrnn.ipynb},
  file = {/home/jsimonrichard/Zotero/storage/N4Q5XGCZ/qgrnn.html}
}

@article{quQuantumConditionalGenerative2023,
  title = {Quantum Conditional Generative Adversarial Network Based on Patch Method for Abnormal Electrocardiogram Generation},
  author = {Qu, Zhiguo and Shi, Wenke and Tiwari, Prayag},
  year = {2023},
  month = sep,
  journal = {Computers in Biology and Medicine},
  pages = {107549},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2023.107549},
  urldate = {2023-10-07},
  abstract = {To address the scarcity and class imbalance of abnormal electrocardiogram (ECG) databases, which are crucial in AI-driven diagnostic tools for potential cardiovascular disease detection, this study proposes a novel quantum conditional generative adversarial algorithm (QCGAN-ECG) for generating abnormal ECG signals. The QCGAN-ECG constructs a quantum generator based on patch method. In this method, each sub-generator generates distinct features of abnormal heartbeats in different segments. This patch-based generative algorithm conserves quantum resources and makes QCGAN-ECG practical for near-term quantum devices. Additionally, QCGAN-ECG introduces quantum registers as control conditions. It encodes information about the types and probability distributions of abnormal heartbeats into quantum registers, rendering the entire generative process controllable. Simulation experiments on Pennylane demonstrated that the QCGAN-ECG could generate completely abnormal heartbeats with an average accuracy of 88.8\%. Moreover, the QCGAN-ECG can accurately fit the probability distribution of various abnormal ECG data. In the anti-noise experiments, the QCGAN-ECG showcased outstanding robustness across various levels of quantum noise interference. These results demonstrate the effectiveness and potential applicability of the QCGAN-ECG for generating abnormal ECG signals, which will further promote the development of AI-driven cardiac disease diagnosis systems.},
  keywords = {Abnormal electrocardiogram,Data imbalance,Generative algorithm,Quantum generative adversarial network},
  file = {/home/jsimonrichard/Zotero/storage/FYN2N5HV/Qu et al. - 2023 - Quantum conditional generative adversarial network.pdf;/home/jsimonrichard/Zotero/storage/Y5N5TWYL/S0010482523010144.html}
}

@article{richardHybridQuantumPurely2024,
  title = {Hybrid {{Quantum}} or {{Purely Classical}}? {{Assessing}} the {{Utility}} of {{Quantum Feature Embeddings}}},
  shorttitle = {Hybrid {{Quantum}} or {{Purely Classical}}?},
  author = {Richard, J. Simon},
  year = {2024},
  month = aug,
  doi = {10.12688/f1000research.154428.1},
  urldate = {2024-08-23},
  abstract = {Background As graph datasets---including social networks, supply chains, and bioinformatics data---grow in size and complexity, researchers are driven to search for solutions enhancing model efficiency and speed. One avenue that may provide a solution is Quantum Graph Learning (QGL), a subfield of Quantum Machine Learning (QML) that applies machine learning inspired or powered by quantum computing to graph learning tasks. Methods We reevaluate Quantum Feature Embeddings (QFE), a QGL methodology published by Xu et al. earlier this year. QFE uses Variational Quantum Circuits to preprocess node features and then sends them to a classical Graph Neural Network (GNN), with the goal of increasing performance and/or decreasing total model size. Xu et al. evaluated this methodology by comparing its performance with the performance of variously-sized classical models on the benchmark datasets PROTEINS and ENZYMES, and they report success. Our core methodology and learning task remain unchanged. However, we have made several changes to the experimental design that enhance the rigor of the study: 1) we include the testing of models with no embedder; 2) we conduct a thorough hyperparameter search using a state-of-the-art optimization algorithm; and 3) we conduct stratified five-fold cross-validation, which mitigates the bias produced by our small datasets and provides multiple test statistics from which we can calculate a confidence interval. Results We produce classical models that perform comparably to QFE and significantly outperform the small classical models used in Xu et al.'s comparison. Notably, many of our classical models achieve this using fewer parameters than the QFE models we trained. Xu et al. do not report their total model sizes. Conclusion Our study sheds doubt on the efficacy of QFE by demonstrating that small, well-tuned classical models can perform just as well as QFE, highlighting the importance of hyperparameter tuning and rigorous experimental design.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Graph Neural Networks,Quantum Feature Embedding,Quantum Graph Learning,Quantum Machine Learning},
  annotation = {[version 1; peer review: 1 approved, 1 approved with reservations]},
  file = {/home/jsimonrichard/Zotero/storage/MD2GBVB7/Richard - 2024 - Hybrid Quantum or Purely Classical Assessing the .pdf}
}

@article{richardMakingTransitionPostQuantum2023,
  title = {Making the {{Transition}} to {{Post-Quantum Cryptography}}},
  author = {Richard, J. Simon},
  year = {2023},
  month = may,
  journal = {The Downtown Review},
  volume = {9},
  number = {2},
  issn = {2381-4292},
  copyright = {All rights reserved},
  file = {/home/jsimonrichard/Zotero/storage/95GNV85G/4.html}
}

@article{richardMakingTransitionPostQuantum2023a,
  title = {Making the {{Transition}} to {{Post-Quantum Cryptography}}},
  author = {Richard, J Simon},
  year = {2023},
  volume = {9},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/69A7MXW5/Richard - 2023 - Making the Transition to Post-Quantum Cryptography.pdf}
}

@misc{richardTrainedModelsCode2024,
  title = {Trained Models, Code, Result Data, and {{Optuna}} Study Data from "{{Hybrid}} Quantum or Purely Classical? {{Assessing}} the Utility of Quantum Feature Embeddings."},
  shorttitle = {Trained Models, Code, Result Data, and {{Optuna}} Study Data from "{{Hybrid}} Quantum or Purely Classical?},
  author = {Richard, J. Simon},
  year = {2024},
  month = jul,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.13117645},
  urldate = {2024-07-30},
  abstract = {This resource contains the model weights, code, result data, and~Optuna study data for "Hybrid quantum or purely classical? Assessing the utility of quantum feature embeddings" written by J. Simon Richard, which critiques "Quantum Feature Embeddings for Graph Neural Networks" written by Xu. et al. The code for this project is also hosted on GitHub at https://github.com/jsimonrichard/QFE-Experiments.},
  langid = {english},
  keywords = {Graph Neural Networks,Quantum Graph Learning,Quantum Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/WV4IGN7Q/13117645.html}
}

@misc{romeroVariationalQuantumGenerators2019,
  title = {Variational Quantum Generators: {{Generative}} Adversarial Quantum Machine Learning for Continuous Distributions},
  shorttitle = {Variational Quantum Generators},
  author = {Romero, Jonathan and {Aspuru-Guzik}, Alan},
  year = {2019},
  month = jan,
  number = {arXiv:1901.00848},
  eprint = {1901.00848},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.00848},
  urldate = {2023-09-30},
  abstract = {We propose a hybrid quantum-classical approach to model continuous classical probability distributions using a variational quantum circuit. The architecture of the variational circuit consists of two parts: a quantum circuit employed to encode a classical random variable into a quantum state, called the quantum encoder, and a variational circuit whose parameters are optimized to mimic a target probability distribution. Samples are generated by measuring the expectation values of a set of operators chosen at the beginning of the calculation. Our quantum generator can be complemented with a classical function, such as a neural network, as part of the classical post-processing. We demonstrate the application of the quantum variational generator using a generative adversarial learning approach, where the quantum generator is trained via its interaction with a discriminator model that compares the generated samples with those coming from the real data distribution. We show that our quantum generator is able to learn target probability distributions using either a classical neural network or a variational quantum circuit as the discriminator. Our implementation takes advantage of automatic differentiation tools to perform the optimization of the variational circuits employed. The framework presented here for the design and implementation of variational quantum generators can serve as a blueprint for designing hybrid quantum-classical architectures for other machine learning tasks on near-term quantum devices.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/9BHXEB3R/Romero and Aspuru-Guzik - 2019 - Variational quantum generators Generative adversa.pdf;/home/jsimonrichard/Zotero/storage/W22LD6SG/1901.html}
}

@misc{Rti02economicimpactsinadequateinfrastructuresoftwaretestingpdf,
  title = {Rti02economicimpactsinadequateinfrastructuresoftwaretesting.Pdf},
  urldate = {2023-10-11},
  howpublished = {https://lara.epfl.ch/w/\_media/misc/rti02economicimpactsinadequateinfrastructuresoftwaretesting.pdf},
  file = {/home/jsimonrichard/Zotero/storage/29I5AX4P/rti02economicimpactsinadequateinfrastructuresoftwaretesting.pdf}
}

@misc{RustCaseStudy,
  title = {Rust {{Case Study}}: {{Community}} Makes {{Rust}} an Easy Choice for Npm},
  urldate = {2023-10-11},
  howpublished = {https://www.rust-lang.org/static/pdfs/Rust-npm-Whitepaper.pdf},
  file = {/home/jsimonrichard/Zotero/storage/HY4EGLSK/Rust-npm-Whitepaper.pdf}
}

@misc{RustVSJava,
  title = {Rust {{VS Java}} Benchmarks, {{Which}} Programming Language or Compiler Is Faster},
  urldate = {2023-11-24},
  howpublished = {https://programming-language-benchmarks.vercel.app/rust-vs-java},
  file = {/home/jsimonrichard/Zotero/storage/PVRNSBQM/rust-vs-java.html}
}

@misc{saniFutureLargeLanguage2024,
  title = {The {{Future}} of {{Large Language Model Pre-training}} Is {{Federated}}},
  author = {Sani, Lorenzo and Iacob, Alex and Cao, Zeyu and Marino, Bill and Gao, Yan and Paulik, Tomas and Zhao, Wanru and Shen, William F. and Aleksandrov, Preslav and Qiu, Xinchi and Lane, Nicholas D.},
  year = {2024},
  month = may,
  number = {arXiv:2405.10853},
  eprint = {2405.10853},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.10853},
  urldate = {2024-06-09},
  abstract = {Generative pre-trained large language models (LLMs) have demonstrated impressive performance over a wide range of tasks, thanks to the unprecedented amount of data they have been trained on. As established scaling laws indicate, LLMs' future performance improvement depends on the amount of computing and data sources we can leverage for pre-training. Federated learning (FL) has the potential to unleash the majority of the planet's data and computational resources, which are underutilized by the data-center-focused training methodology of current LLM practice. Our work presents a robust, flexible, reproducible FL approach that enables large-scale collaboration across institutions to train LLMs. This would mobilize more computational and data resources while matching or potentially exceeding centralized performance. We further show the effectiveness of the federated training scales with model size and present our approach for training a billion-scale federated LLM using limited resources. This will help data-rich actors to become the protagonists of LLMs pre-training instead of leaving the stage to compute-rich actors alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/3UMMW4AX/Sani et al. - 2024 - The Future of Large Language Model Pre-training is.pdf;/home/jsimonrichard/Zotero/storage/AGUU3X57/2405.html}
}

@article{schomburgBRENDAEnzymeDatabase2004,
  title = {{{BRENDA}}, the Enzyme Database: Updates and Major New Developments},
  shorttitle = {{{BRENDA}}, the Enzyme Database},
  author = {Schomburg, Ida and Chang, Antje and Ebeling, Christian and Gremse, Marion and Heldt, Christian and Huhn, Gregor and Schomburg, Dietmar},
  year = {2004},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {32},
  number = {suppl\_1},
  pages = {D431-D433},
  issn = {0305-1048},
  doi = {10.1093/nar/gkh081},
  urldate = {2024-05-04},
  abstract = {BRENDA (BRaunschweig ENzyme DAtabase) represents a comprehensive collection of enzyme and metabolic information, based on primary literature. The database contains data from at least 83 000 different enzymes from 9800 different organisms, classified in {$\sim$}4200 EC numbers. BRENDA includes biochemical and molecular information on classification and nomenclature, reaction and specificity, functional parameters, occurrence, enzyme structure, application, engineering, stability, disease, isolation and preparation, links and literature references. The data are extracted and evaluated from {$\sim$}46 000 references, which are linked to PubMed as long as the reference is cited in PubMed. In the past year BRENDA has undergone major changes including a large increase in updating speed with \&gt;50\% of all data updated in 2002 or in the first half of 2003, the development of a new EC-tree browser, a taxonomy-tree browser, a chemical substructure search engine for ligand structure, the development of controlled vocabulary, an ontology for some information fields and a thesaurus for ligand names. The database is accessible free of charge to the academic community at http://www.brenda. uni-koeln.de .},
  file = {/home/jsimonrichard/Zotero/storage/U4GCQUQG/Schomburg et al. - 2004 - BRENDA, the enzyme database updates and major new.pdf;/home/jsimonrichard/Zotero/storage/GHKWUUIF/2505276.html}
}

@book{schuldMachineLearningQuantum2021,
  title = {Machine {{Learning}} with {{Quantum Computers}}},
  author = {Schuld, Maria and Petruccione, Francesco},
  year = {2021},
  series = {Quantum {{Science}} and {{Technology}}},
  edition = {2},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-83098-4},
  urldate = {2023-09-30},
  isbn = {978-3-030-83097-7 978-3-030-83098-4},
  langid = {english},
  keywords = {quantum machine learning},
  file = {/home/jsimonrichard/Zotero/storage/TJFF74YD/Second Edition.pdf}
}

@article{ScientistsMakeSevenbit2000,
  title = {Scientists Make Seven-Bit Quantum Leap in Computer Research},
  year = {2000},
  month = mar,
  journal = {MIT News},
  urldate = {2022-12-03}
}

@inproceedings{seaboldStatsmodelsEconometricStatistical2010,
  title = {Statsmodels: {{Econometric}} and {{Statistical Modeling}} with {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {Seabold, Skipper and Perktold, Josef},
  editor = {{van der Walt}, St{\'e}fan and Millman, Jarrod},
  year = {2010},
  pages = {92--96},
  doi = {10.25080/Majora-92bf1922-011}
}

@book{sebestaConceptsProgrammingLanguages2019,
  title = {Concepts of {{Programming Languages}}},
  author = {Sebesta, Robert W.},
  year = {2019},
  edition = {12},
  publisher = {Pearson}
}

@article{sedkowskiQuantumRace2021,
  title = {Quantum {{Race}}},
  author = {S{\k e}dkowski, Wiktor},
  year = {2021},
  month = nov,
  journal = {Warsaw Institute},
  urldate = {2022-11-22}
}

@article{shaibEfficientNoiseMitigation2023,
  title = {Efficient Noise Mitigation Technique for Quantum Computing},
  author = {Shaib, Ali and Naim, Mohamad Hussein and Fouda, Mohammed E. and Kanj, Rouwaida and Kurdahi, Fadi},
  year = {2023},
  month = mar,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {3912},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-30510-5},
  urldate = {2023-09-30},
  abstract = {Quantum computers have enabled solving problems beyond the current machines' capabilities. However, this requires handling noise arising from unwanted interactions in these systems. Several protocols have been proposed to address efficient and accurate quantum noise profiling and mitigation. In this work, we propose a novel protocol that efficiently estimates the average output of a noisy quantum device to be used for quantum noise mitigation. The multi-qubit system average behavior is approximated as a special form of a Pauli Channel where Clifford gates are used to estimate the average output for circuits of different depths. The characterized Pauli channel error rates, and state preparation and measurement errors are then used to construct the outputs for different depths thereby eliminating the need for large simulations and enabling efficient mitigation. We demonstrate the efficiency of the proposed protocol on four IBM Q 5-qubit quantum devices. Our method demonstrates improved accuracy with efficient noise characterization. We report up to 88\% and 69\% improvement for the proposed approach compared to the unmitigated, and pure measurement error mitigation approaches, respectively.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Electrical and electronic engineering,Quantum information},
  file = {/home/jsimonrichard/Zotero/storage/ZIU76CTW/Shaib et al. - 2023 - Efficient noise mitigation technique for quantum c.pdf}
}

@article{shaoQuantumAlgorithmDesign2019,
  title = {Quantum {{Algorithm Design}}: {{Techniques}} and {{Applications}}},
  shorttitle = {Quantum {{Algorithm Design}}},
  author = {Shao, Changpeng and Li, Yang and Li, Hongbo},
  year = {2019},
  month = feb,
  journal = {Journal of Systems Science and Complexity},
  volume = {32},
  number = {1},
  pages = {375--452},
  issn = {1559-7067},
  doi = {10.1007/s11424-019-9008-0},
  urldate = {2023-09-11},
  abstract = {In recent years, rapid developments of quantum computer are witnessed in both the hardware and the algorithm domains, making it necessary to have an updated review of some major techniques and applications in quantum algorithm design.},
  langid = {english},
  keywords = {Quantum algorithm,quantum computation,quantum machine learning,quantum search,quantum walk},
  file = {/home/jsimonrichard/Zotero/storage/37NM9FPJ/Shao et al. - 2019 - Quantum Algorithm Design Techniques and Applicati.pdf}
}

@article{shawQuantumOutlook20192018,
  title = {Quantum {{Outlook}} 2019},
  author = {Shaw, David},
  year = {2018},
  month = dec,
  journal = {Fact Based Insight},
  urldate = {2022-11-23}
}

@article{shenQuantumFourierConvolutional2023,
  title = {Quantum {{Fourier Convolutional Network}}},
  author = {Shen, Feihong and Liu, Jun},
  year = {2023},
  month = jan,
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  volume = {19},
  number = {1},
  pages = {13:1--13:14},
  issn = {1551-6857},
  doi = {10.1145/3514249},
  urldate = {2024-04-15},
  abstract = {The neural network and quantum computing are both significant and appealing fields, with their interactive disciplines promising for large-scale computing tasks that are untackled by conventional computers. However, both developments are restricted by the scope of the hardware development. Nevertheless, many neural network algorithms had been proposed before GPUs became powerful enough for running very deep models. Similarly, quantum algorithms can also be proposed as knowledge reserve before real quantum computers are easily accessible. Specifically, taking advantage of both the neural networks and quantum computation and designing quantum deep neural networks (QDNNs) for acceleration on the Noisy Intermediate-Scale Quantum (NISQ) processors is also an important research problem. As one of the most widely used neural network architectures, convolutional neural network (CNN) remains to be accelerated by quantum mechanisms, with only a few attempts having been demonstrated. In this article, we propose a new hybrid quantum-classical circuit, namely, Quantum Fourier Convolutional Network (QFCN). Our model achieves exponential speedup compared with classical CNN theoretically and improves over the existing best result of quantum CNN. We demonstrate the potential of this architecture by applying it on different deep learning tasks, including traffic prediction and image classification.},
  keywords = {convolutional neural network,hybrid quantum-classical circuit,Quantum machine learning},
  file = {/home/jsimonrichard/Zotero/storage/Y3E4RDSH/Shen and Liu - 2023 - Quantum Fourier Convolutional Network.pdf}
}

@article{shenQuantumFourierConvolutional2023a,
  title = {Quantum {{Fourier Convolutional Network}}},
  author = {Shen, Feihong and Liu, Jun},
  year = {2023},
  month = jan,
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  volume = {19},
  number = {1},
  pages = {13:1--13:14},
  issn = {1551-6857},
  doi = {10.1145/3514249},
  urldate = {2024-01-21},
  abstract = {The neural network and quantum computing are both significant and appealing fields, with their interactive disciplines promising for large-scale computing tasks that are untackled by conventional computers. However, both developments are restricted by the scope of the hardware development. Nevertheless, many neural network algorithms had been proposed before GPUs became powerful enough for running very deep models. Similarly, quantum algorithms can also be proposed as knowledge reserve before real quantum computers are easily accessible. Specifically, taking advantage of both the neural networks and quantum computation and designing quantum deep neural networks (QDNNs) for acceleration on the Noisy Intermediate-Scale Quantum (NISQ) processors is also an important research problem. As one of the most widely used neural network architectures, convolutional neural network (CNN) remains to be accelerated by quantum mechanisms, with only a few attempts having been demonstrated. In this article, we propose a new hybrid quantum-classical circuit, namely, Quantum Fourier Convolutional Network (QFCN). Our model achieves exponential speedup compared with classical CNN theoretically and improves over the existing best result of quantum CNN. We demonstrate the potential of this architecture by applying it on different deep learning tasks, including traffic prediction and image classification.},
  keywords = {convolutional neural network,hybrid quantum-classical circuit,Quantum machine learning},
  file = {/home/jsimonrichard/Zotero/storage/SYPVQJ8N/Shen and Liu - 2023 - Quantum Fourier Convolutional Network.pdf}
}

@inproceedings{shorAlgorithmsQuantumComputation1994,
  title = {Algorithms for Quantum Computation: {{Discrete}} Logarithms and Factoring},
  booktitle = {Proceedings - {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}, {{FOCS}}},
  author = {Shor, Peter W.},
  year = {1994},
  issn = {02725428},
  doi = {10.1109/SFCS.1994.365700},
  abstract = {A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in computation time of at most a polynomial factor. It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their computational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. (We thus give the first examples of quantum cryptanalysis).}
}

@article{shumailovAIModelsCollapse2024,
  title = {{{AI}} Models Collapse When Trained on Recursively Generated Data},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  year = {2024},
  month = jul,
  journal = {Nature},
  volume = {631},
  number = {8022},
  pages = {755--759},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07566-y},
  urldate = {2024-08-17},
  abstract = {Stable diffusion revolutionized image creation from descriptive text. GPT-2 (ref.\,1), GPT-3(.5) (ref.\,2) and GPT-4 (ref.\,3) demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here we consider what may happen to GPT-\{n\} once LLMs contribute much of the text found online. We find that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. We refer to this effect as `model collapse' and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). We build theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. We demonstrate that it must be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/home/jsimonrichard/Zotero/storage/5PB7KLBA/Shumailov et al. - 2024 - AI models collapse when trained on recursively gen.pdf}
}

@article{sierpinskiProprieteTopologiqueEnsembles1920,
  title = {{Sur une propri{\'e}t{\'e} topologique des ensembles d{\'e}nombrables denses en soi}},
  author = {Sierpi{\'n}ski, Wac{\l}aw},
  year = {1920},
  journal = {Fundamenta Mathematicae},
  volume = {1},
  pages = {11--16},
  publisher = {Instytut Matematyczny Polskiej Akademii Nauk},
  issn = {0016-2736, 1730-6329},
  doi = {10.4064/fm-1-1-11-16},
  urldate = {2024-04-20},
  abstract = {Le but de cette note est de d{\'e}montrer le th{\'e}or{\`e}me suivant: Tout les ensembles d{\'e}nombrables denses en soi (situ{\'e} dans un espace euclidien {\`a} un nombre quelconque de dimension) sont hom{\'e}omorphes.},
  langid = {polish},
  file = {/home/jsimonrichard/Zotero/storage/Y3Q8Q6PG/Sierpiński - 1920 - Sur une propriété topologique des ensembles dénomb.pdf}
}

@article{singmanChinaPosesBiggest2022,
  title = {China Poses 'biggest Long-Term Threat to Economic and National Security,' {{FBI Director Wray}} Warns},
  author = {Singman, Brooke},
  year = {2022},
  month = jul,
  journal = {Fox News},
  urldate = {2022-11-23}
}

@article{smith-goodsonQuantumComputerBattle2019,
  title = {Quantum {{Computer Battle Royale}}: {{Upstart Ions Versus Old Guard Superconductors}}},
  author = {{Smith-Goodson}, Paul},
  year = {2019},
  month = sep,
  journal = {Forbes},
  urldate = {2022-12-03}
}

@techreport{SoftwareSupportingUS,
  title = {Software: {{Supporting US Through COVID}} (2021)},
  shorttitle = {Software},
  institution = {BSA Foundation},
  urldate = {2023-10-14},
  abstract = {Our new report demonstrates how software has helped businesses shift their models and promote public health during the pandemic.},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/WLVRUZFY/Software Supporting US Through COVID.pdf;/home/jsimonrichard/Zotero/storage/VPFMXGN9/software-supporting-us-through-covid-2021.html}
}

@misc{StackOverflowDeveloper,
  title = {Stack {{Overflow Developer Survey}} 2023},
  journal = {Stack Overflow},
  urldate = {2023-10-10},
  abstract = {In May 2023 over 90,000 developers responded to our annual survey about how they learn and level up, which tools they're using, and which ones they want.},
  howpublished = {https://survey.stackoverflow.co/2023/?utm\_source=social-share\&utm\_medium=social\&utm\_campaign=dev-survey-2023},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/XXJKCMSG/2023.html}
}

@incollection{stroustrupHistory197919911996,
  title = {A History of {{C}}++: 1979--1991},
  shorttitle = {A History of {{C}}++},
  booktitle = {History of Programming Languages---{{II}}},
  author = {Stroustrup, Bjarne},
  year = {1996},
  month = jan,
  pages = {699--769},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  urldate = {2023-10-17},
  abstract = {This paper outlines the history of the C++ programming language. The emphasis is on the ideas, constraints, and people that shaped the language, rather than the minutiae of language features. Key design decisions relating to language features are discussed, but the focus is one the overall design goals and practical constraints. The evolution of C++ is traced from C with Classes to the current ANSI and ISO standards work and the explosion of use, interest, commercial activity, compilers, tools, environments, and libraries.},
  isbn = {978-0-201-89502-5},
  file = {/home/jsimonrichard/Zotero/storage/RQYG7F5E/Stroustrup - 1996 - A history of C++ 1979--1991.pdf}
}

@article{sunMitigatingRealisticNoise2021,
  title = {Mitigating {{Realistic Noise}} in {{Practical Noisy Intermediate-Scale Quantum Devices}}},
  author = {Sun, Jinzhao and Yuan, Xiao and Tsunoda, Takahiro and Vedral, Vlatko and Benjamin, Simon C. and Endo, Suguru},
  year = {2021},
  month = mar,
  journal = {Physical Review Applied},
  volume = {15},
  number = {3},
  pages = {034026},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevApplied.15.034026},
  urldate = {2023-09-30},
  abstract = {Quantum error mitigation (QEM) is vital for noisy intermediate-scale quantum (NISQ) devices. While most conventional QEM schemes assume discrete gate-based circuits with noise appearing either before or after each gate, the assumptions are inappropriate for describing realistic noise that may have strong gate dependence and complicated nonlocal effects, and general computing models such as analog quantum simulators. To address these challenges, we first extend the scenario, where each computation process, being either digital or analog, is described by a continuous time evolution. For noise from imperfections of the engineered Hamiltonian or additional noise operators, we show it can be effectively suppressed by a stochastic QEM method. Since our method assumes only accurate single qubit controls, it is applicable to all digital quantum computers and various analog simulators. Meanwhile, errors in the mitigation procedure can be suppressed by leveraging the Richardson extrapolation method. As we numerically test our method with various Hamiltonians under energy relaxation and dephasing noise and digital quantum circuits with additional two-qubit crosstalk, we show an improvement of simulation accuracy by 2 orders. We assess the resource cost of our scheme and conclude the feasibility of accurate quantum computing with NISQ devices.},
  file = {/home/jsimonrichard/Zotero/storage/HXF3MNEU/Sun et al. - 2021 - Mitigating Realistic Noise in Practical Noisy Inte.pdf;/home/jsimonrichard/Zotero/storage/IDZBLJVI/PhysRevApplied.15.html}
}

@article{tianRecentAdvancesQuantum2023,
  title = {Recent {{Advances}} for {{Quantum Neural Networks}} in {{Generative Learning}}},
  author = {Tian, Jinkai and Sun, Xiaoyu and Du, Yuxuan and Zhao, Shanshan and Liu, Qing and Zhang, Kaining and Yi, Wei and Huang, Wanrong and Wang, Chaoyue and Wu, Xingyao and Hsieh, Min-Hsiu and Liu, Tongliang and Yang, Wenjing and Tao, Dacheng},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {10},
  pages = {12321--12340},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3272029},
  urldate = {2023-10-07},
  abstract = {Quantum computers are next-generation devices that hold promise to perform calculations beyond the reach of classical computers. A leading method towards achieving this goal is through quantum machine learning, especially quantum generative learning. Due to the intrinsic probabilistic nature of quantum mechanics, it is reasonable to postulate that quantum generative learning models (QGLMs) may surpass their classical counterparts. As such, QGLMs are receiving growing attention from the quantum physics and computer science communities, where various QGLMs that can be efficiently implemented on near-term quantum machines with potential computational advantages are proposed. In this paper, we review the current progress of QGLMs from the perspective of machine learning. Particularly, we interpret these QGLMs, covering quantum circuit Born machines, quantum generative adversarial networks, quantum Boltzmann machines, and quantum variational autoencoders, as the quantum extension of classical generative learning models. In this context, we explore their intrinsic relations and their fundamental differences. We further summarize the potential applications of QGLMs in both conventional machine learning tasks and quantum physics. Last, we discuss the challenges and further research directions for QGLMs.},
  file = {/home/jsimonrichard/Zotero/storage/4RMJT97T/Tian et al. - 2023 - Recent Advances for Quantum Neural Networks in Gen.pdf;/home/jsimonrichard/Zotero/storage/L547PELV/10113742.html}
}

@article{tsubakiCorrectionFastAccurate2019,
  title = {Correction to ``{{Fast}} and {{Accurate Molecular Property Prediction}}: {{Learning Atomic Interactions}} and {{Potentials}} with {{Neural Networks}}''},
  shorttitle = {Correction to ``{{Fast}} and {{Accurate Molecular Property Prediction}}},
  author = {Tsubaki, Masashi and Mizoguchi, Teruyasu},
  year = {2019},
  month = may,
  journal = {The Journal of Physical Chemistry Letters},
  volume = {10},
  number = {9},
  pages = {2066--2067},
  publisher = {American Chemical Society},
  doi = {10.1021/acs.jpclett.9b00301},
  urldate = {2023-12-30},
  file = {/home/jsimonrichard/Zotero/storage/E5487X7D/Tsubaki and Mizoguchi - 2019 - Correction to “Fast and Accurate Molecular Propert.pdf}
}

@article{tsubakiFastAccurateMolecular2018,
  title = {Fast and {{Accurate Molecular Property Prediction}}: {{Learning Atomic Interactions}} and {{Potentials}} with {{Neural Networks}}},
  shorttitle = {Fast and {{Accurate Molecular Property Prediction}}},
  author = {Tsubaki, Masashi and Mizoguchi, Teruyasu},
  year = {2018},
  month = oct,
  journal = {The Journal of Physical Chemistry Letters},
  volume = {9},
  number = {19},
  pages = {5733--5741},
  publisher = {American Chemical Society},
  doi = {10.1021/acs.jpclett.8b01837},
  urldate = {2023-12-30},
  abstract = {The discovery of molecules with specific properties is crucial to developing effective materials and useful drugs. Recently, to accelerate such discoveries with machine learning, deep neural networks (DNNs) have been applied to quantum chemistry calculations based on the density functional theory (DFT). While various DNNs for quantum chemistry have been proposed, these networks require various chemical descriptors as inputs and a large number of learning parameters to model atomic interactions. In this paper, we propose a new DNN-based molecular property prediction that (i) does not depend on descriptors, (ii) is more compact, and (iii) involves additional neural networks to model the interactions between all the atoms in a molecular structure. In the consideration of the molecular structure, we also model the potentials between all the atoms; this allows the neural networks to simultaneously learn the atomic interactions and potentials. We emphasize that these atomic ``pair'' interactions and potentials are characterized using the global molecular structure, a function of the depth of the neural networks; this leads to the implicit or indirect consideration of atomic ``many-body'' interactions and potentials within the DNNs. In the evaluation of our model with the benchmark QM9 data set, we achieved fast and accurate prediction performances for various quantum chemical properties. In addition, we analyzed the effects of learning the interactions and potentials on each property. Furthermore, we demonstrated an extrapolation evaluation, i.e., we trained a model with small molecules and tested it with large molecules. We believe that insights into the extrapolation evaluation will be useful for developing more practical applications in DNN-based molecular property predictions.}
}

@article{tycholaQuantumMachineLearning2023,
  title = {Quantum {{Machine Learning}}---{{An Overview}}},
  author = {Tychola, Kyriaki A. and Kalampokas, Theofanis and Papakostas, George A.},
  year = {2023},
  month = jan,
  journal = {Electronics},
  volume = {12},
  number = {11},
  pages = {2379},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics12112379},
  urldate = {2024-05-04},
  abstract = {Quantum computing has been proven to excel in factorization issues and unordered search problems due to its capability of quantum parallelism. This unique feature allows exponential speed-up in solving certain problems. However, this advantage does not apply universally, and challenges arise when combining classical and quantum computing to achieve acceleration in computation speed. This paper aims to address these challenges by exploring the current state of quantum machine learning and benchmarking the performance of quantum and classical algorithms in terms of accuracy. Specifically, we conducted experiments with three datasets for binary classification, implementing Support Vector Machine (SVM) and Quantum SVM (QSVM) algorithms. Our findings suggest that the QSVM algorithm outperforms classical SVM on complex datasets, and the performance gap between quantum and classical models increases with dataset complexity, as simple models tend to overfit with complex datasets. While there is still a long way to go in terms of developing quantum hardware with sufficient resources, quantum machine learning holds great potential in areas such as unsupervised learning and generative models. Moving forward, more efforts are needed to explore new quantum learning models that can leverage the power of quantum mechanics to overcome the limitations of classical machine learning.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {quantum classifier,quantum computer,quantum machine learning,quantum support vector machine},
  file = {/home/jsimonrichard/Zotero/storage/WP6E6L3I/Tychola et al. - 2023 - Quantum Machine Learning—An Overview.pdf}
}

@article{tycholaQuantumMachineLearning2023a,
  title = {Quantum {{Machine Learning}}---{{An Overview}}},
  author = {Tychola, Kyriaki A. and Kalampokas, Theofanis and Papakostas, George A.},
  year = {2023},
  month = jan,
  journal = {Electronics},
  volume = {12},
  number = {11},
  pages = {2379},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics12112379},
  urldate = {2024-03-07},
  abstract = {Quantum computing has been proven to excel in factorization issues and unordered search problems due to its capability of quantum parallelism. This unique feature allows exponential speed-up in solving certain problems. However, this advantage does not apply universally, and challenges arise when combining classical and quantum computing to achieve acceleration in computation speed. This paper aims to address these challenges by exploring the current state of quantum machine learning and benchmarking the performance of quantum and classical algorithms in terms of accuracy. Specifically, we conducted experiments with three datasets for binary classification, implementing Support Vector Machine (SVM) and Quantum SVM (QSVM) algorithms. Our findings suggest that the QSVM algorithm outperforms classical SVM on complex datasets, and the performance gap between quantum and classical models increases with dataset complexity, as simple models tend to overfit with complex datasets. While there is still a long way to go in terms of developing quantum hardware with sufficient resources, quantum machine learning holds great potential in areas such as unsupervised learning and generative models. Moving forward, more efforts are needed to explore new quantum learning models that can leverage the power of quantum mechanics to overcome the limitations of classical machine learning.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {quantum classifier,quantum computer,quantum machine learning,quantum support vector machine},
  file = {/home/jsimonrichard/Zotero/storage/VWABFSEW/Tychola et al. - 2023 - Quantum Machine Learning—An Overview.pdf}
}

@article{uenuma20YearsLater2019,
  title = {20 {{Years Later}}, the {{Y2K Bug Seems Like}} a {{Joke}}---{{Because Those Behind}} the {{Scenes Took It Seriously}}},
  author = {Uenuma, Francine},
  year = {2019},
  month = dec,
  journal = {Time},
  urldate = {2022-12-05}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10903},
  urldate = {2024-04-26},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/YFPXS5Z3/Veličković et al. - 2018 - Graph Attention Networks.pdf;/home/jsimonrichard/Zotero/storage/Y2NFM7AF/1710.html}
}

@misc{verdonQuantumGraphNeural2019,
  title = {Quantum {{Graph Neural Networks}}},
  author = {Verdon, Guillaume and McCourt, Trevor and Luzhnica, Enxhell and Singh, Vikash and Leichenauer, Stefan and Hidary, Jack},
  year = {2019},
  month = sep,
  number = {arXiv:1909.12264},
  eprint = {1909.12264},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.12264},
  urldate = {2023-11-12},
  abstract = {We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNNs: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantum Physics},
  file = {/home/jsimonrichard/Zotero/storage/KIPKDVSJ/Verdon et al. - 2019 - Quantum Graph Neural Networks.pdf;/home/jsimonrichard/Zotero/storage/82AKPMSU/1909.html}
}

@misc{vieiraMugTorusMorphgif2007,
  title = {Mug and {{Torus}} Morph.Gif},
  shorttitle = {English},
  author = {Vieira, Lucas},
  year = {2007},
  month = mar,
  urldate = {2024-04-18},
  copyright = {Public domain},
  file = {/home/jsimonrichard/Zotero/storage/D324BQGK/FileMug_and_Torus_morph.html}
}

@article{virtanenSciPy10Fundamental2020,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  shorttitle = {{{SciPy}} 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  urldate = {2024-05-06},
  abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments. This Perspective describes the development and capabilities of SciPy 1.0, an open source scientific computing library for the Python programming language.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Biophysical chemistry,Computational biology and bioinformatics,Technology},
  file = {/home/jsimonrichard/Zotero/storage/7QG2529G/Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf}
}

@book{willardGeneralTopology2004,
  title = {General {{Topology}}},
  author = {Willard, Stephen},
  year = {2004},
  month = jan,
  publisher = {Courier Corporation},
  abstract = {Among the best available reference introductions to general topology, this volume is appropriate for advanced undergraduate and beginning graduate students. Its treatment encompasses two broad areas of topology: "continuous topology," represented by sections on convergence, compactness, metrization and complete metric spaces, uniform spaces, and function spaces; and "geometric topology," covered by nine sections on connectivity properties, topological characterization theorems, and homotopy theory. Many standard spaces are introduced in the related problems that accompany each section (340 exercises in all). The text's value as a reference work is enhanced by a collection of historical notes, a bibliography, and index. 1970 edition. 27 figures.},
  isbn = {978-0-486-43479-7},
  langid = {english},
  keywords = {Mathematics / Topology}
}

@article{wilsonProbableInferenceLaw1927,
  title = {Probable {{Inference}}, the {{Law}} of {{Succession}}, and {{Statistical Inference}}},
  author = {Wilson, Edwin B.},
  year = {1927},
  month = jun,
  journal = {Journal of the American Statistical Association},
  publisher = {Taylor \& Francis Group},
  urldate = {2024-05-12},
  abstract = {Published in Journal of the American Statistical Association (Vol. 22, No. 158, 1927)},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english},
  file = {/home/jsimonrichard/Zotero/storage/SMWRUBS3/01621459.1927.html}
}

@inproceedings{woodSpecialSessionNoise2020,
  title = {Special {{Session}}: {{Noise Characterization}} and {{Error Mitigation}} in {{Near-Term Quantum Computers}}},
  shorttitle = {Special {{Session}}},
  booktitle = {2020 {{IEEE}} 38th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Wood, Christopher J.},
  year = {2020},
  month = oct,
  pages = {13--16},
  issn = {2576-6996},
  doi = {10.1109/ICCD50377.2020.00016},
  urldate = {2023-09-30},
  abstract = {A detailed understanding of the noise processes and errors in near-term quantum computers is essential for accurate calibration of gates, and for achieving optimal performance when executing near-term applications such as quantum chemistry, optimization, or machine learning problems. This paper introduces several sources of noise in superconducting qubit based quantum computers such as those deployed by IBM, describes some of the main techniques for characterizing the resulting errors, and for mitigating the effects of certain errors without access to quantum error correction. We demonstrate experimental application of these techniques on an IBM Quantum system using the open source Qiskit software library.},
  file = {/home/jsimonrichard/Zotero/storage/CA33RGVR/Wood - 2020 - Special Session Noise Characterization and Error .pdf;/home/jsimonrichard/Zotero/storage/L6ANSYE7/9283531.html}
}

@article{xi-hanQuantumSecureDirect2007,
  title = {Quantum Secure Direct Communication with Quantum Encryption Based on Pure Entangled States},
  author = {{Xi-Han}, Li and {Chun-Yan}, Li and {Fu-Guo}, Deng and Ping, Zhou and {Yu-Jie}, Liang and {Hong-Yu}, Zhou},
  year = {2007},
  journal = {Chinese Physics},
  volume = {16},
  number = {8},
  pages = {2149},
  publisher = {IOP Publishing}
}

@misc{xuPrivateCorrespondence2024,
  title = {Private {{Correspondence}}},
  author = {Xu, Sascha},
  year = {2024},
  month = jul
}

@inproceedings{xuQuantumFeatureEmbeddings2024,
  title = {Quantum {{Feature Embeddings}} for {{Graph Neural Networks}}},
  booktitle = {Proceedings of the 57th {{Hawaii International Conference}} on {{System Sciences}}},
  author = {Xu, Sascha and {Wilhelm-Mauch}, Frank and Maass, Wolfgang},
  year = {2024},
  month = jan,
  urldate = {2024-01-21},
  abstract = {Quantum computing offers a promising avenue to reduce growing machine learning model complexity as required in large language models and simulation models for weather forecasts, financial forecasts, or engineering. Graph neural networks are a particular class of machine learning models that have garnered much attention for their ability to deal well with structured data. We investigate how to enhance existing GNNs and find through the inductive bias that quantum circuits are used best to encode node features. The proposed Quantum Feature Embeddings (QFEs) turn raw input features into quantum states, enabling non-linear and entangled representations. In particular, QFEs provide normalized, non-redundant weight matrices in an exponentially larger feature space and require much fewer qubits than fully quantum graph neural networks. On standard graph benchmark datasets, we showcase that for the same parameter count QFEs perform better than their classical counterpart, and are able to match the performance of an exponentially larger model. Finally, we study the potential benefit of using a hybrid quantum graph neural network over a classic alternative on a concrete use case, laser cutting. We find that the proposed model has the performance and thus the near-term potential to uplift these business applications.},
  isbn = {978-0-9981331-7-1},
  langid = {english},
  keywords = {PROTEINS Dataset},
  file = {/home/jsimonrichard/Zotero/storage/GZUGZMDA/Xu et al. - 2024 - Quantum Feature Embeddings for Graph Neural Networ.pdf;/home/jsimonrichard/Zotero/storage/QATI8JJS/Xu et al. - Quantum Feature Embeddings for Graph Neural Networ.pdf}
}

@misc{Y2KBug,
  title = {{{Y2K Bug}}},
  publisher = {Nostalgia Central},
  urldate = {2022-12-05},
  howpublished = {https://nostalgiacentral.com/pop-culture/fads/y2k-bug/}
}

@misc{yangRevisitingSemiSupervisedLearning2016,
  title = {Revisiting {{Semi-Supervised Learning}} with {{Graph Embeddings}}},
  author = {Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
  year = {2016},
  month = may,
  number = {arXiv:1603.08861},
  eprint = {1603.08861},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.08861},
  urldate = {2024-02-02},
  abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/MJ2GEC5B/Yang et al. - 2016 - Revisiting Semi-Supervised Learning with Graph Emb.pdf;/home/jsimonrichard/Zotero/storage/J5NU5CER/1603.html}
}

@misc{Year2000Information1998,
  title = {Year 2000 {{Information}} and {{Readiness Disclosure Act}}},
  year = {1998},
  address = {Public Law 105-271}
}

@misc{Year2000Problem1998,
  title = {The {{Year}} 2000 {{Problem}}: {{Fourth Report}} by the {{Committee}} on {{Government Reform}} and {{Oversight}}},
  year = {1998},
  month = oct,
  publisher = {U.S. House of Representatives},
  urldate = {2022-12-05}
}

@misc{yuQuantumGraphLearning2023,
  title = {Quantum {{Graph Learning}}: {{Frontiers}} and {{Outlook}}},
  shorttitle = {Quantum {{Graph Learning}}},
  author = {Yu, Shuo and Peng, Ciyuan and Wang, Yingbo and Shehzad, Ahsan and Xia, Feng and Hancock, Edwin R.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00892},
  eprint = {2302.00892},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00892},
  urldate = {2024-02-19},
  abstract = {Quantum theory has shown its superiority in enhancing machine learning. However, facilitating quantum theory to enhance graph learning is in its infancy. This survey investigates the current advances in quantum graph learning (QGL) from three perspectives, i.e., underlying theories, methods, and prospects. We first look at QGL and discuss the mutualism of quantum theory and graph learning, the specificity of graph-structured data, and the bottleneck of graph learning, respectively. A new taxonomy of QGL is presented, i.e., quantum computing on graphs, quantum graph representation, and quantum circuits for graph neural networks. Pitfall traps are then highlighted and explained. This survey aims to provide a brief but insightful introduction to this emerging field, along with a detailed discussion of frontiers and outlook yet to be investigated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Summary},
  file = {/home/jsimonrichard/Zotero/storage/4FQI46CJ/Yu et al. - 2023 - Quantum Graph Learning Frontiers and Outlook.pdf;/home/jsimonrichard/Zotero/storage/4HCE99KG/2302.html}
}

@misc{zhangHierarchicalGraphPooling2019,
  title = {Hierarchical {{Graph Pooling}} with {{Structure Learning}}},
  author = {Zhang, Zhen and Bu, Jiajun and Ester, Martin and Zhang, Jianfeng and Yao, Chengwei and Yu, Zhi and Wang, Can},
  year = {2019},
  month = dec,
  number = {arXiv:1911.05954},
  eprint = {1911.05954},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.05954},
  urldate = {2024-04-21},
  abstract = {Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jsimonrichard/Zotero/storage/EMXLVVUK/Zhang et al. - 2019 - Hierarchical Graph Pooling with Structure Learning.pdf;/home/jsimonrichard/Zotero/storage/TBX3KPFW/1911.html}
}

@article{zhangHierarchicalMultiViewGraph2023,
  title = {Hierarchical {{Multi-View Graph Pooling With Structure Learning}}},
  author = {Zhang, Zhen and Bu, Jiajun and Ester, Martin and Zhang, Jianfeng and Li, Zhao and Yao, Chengwei and Dai, Huifen and Yu, Zhi and Wang, Can},
  year = {2023},
  month = jan,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {1},
  pages = {545--559},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3090664},
  urldate = {2024-04-21},
  abstract = {Graph Neural Networks (GNNs), whch generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we proposed a novel multi-view graph pooling operator dubbed as MVPool, which ranks nodes across different views with different contextual graph information. Meanwhile, attention mechanism is utilized to promote the collaboration of different views for generating robust node rankings. Then the pooling operation adaptively selects a subset of nodes to form an induced subgraph based on the ranking list. To preserve the underlying graph topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. The proposed MVPool operator is a general strategy that can be integrated into various graph neural network architectures, including GCN, GAT and GraphSAGE, etc. By combining MVPool operator with graph neural networks, we perform hierarchical representation learning for both node and graph level classification as well as clustering tasks. Experimental results on thirteen widely used transductive and inductive benchmarks demonstrate the effectiveness of our proposed model.},
  keywords = {clustering,Collaboration,Convolution,deep learning,Electronic mail,graph classification,Graph neural networks,graph pooling,Learning systems,Message passing,node classification,Task analysis},
  file = {/home/jsimonrichard/Zotero/storage/38AZVSMA/Zhang et al. - 2023 - Hierarchical Multi-View Graph Pooling With Structu.pdf;/home/jsimonrichard/Zotero/storage/TW4LKDUU/9460814.html}
}

@article{zhangInteractiveContinuousCollision2006,
  title = {Interactive Continuous Collision Detection for Non-Convex Polyhedra},
  author = {Zhang, Xinyu and Lee, Minkyoung and Kim, Young J.},
  year = {2006},
  month = sep,
  journal = {The Visual Computer},
  volume = {22},
  number = {9-11},
  pages = {749--760},
  issn = {0178-2789},
  doi = {10.1007/s00371-006-0060-0},
  urldate = {2022-12-22}
}

@misc{zhangMolecularMechanicsDrivenGraph2020,
  title = {Molecular {{Mechanics-Driven Graph Neural Network}} with {{Multiplex Graph}} for {{Molecular Structures}}},
  author = {Zhang, Shuo and Liu, Yang and Xie, Lei},
  year = {2020},
  month = nov,
  number = {arXiv:2011.07457},
  eprint = {2011.07457},
  primaryclass = {physics, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.07457},
  urldate = {2023-10-27},
  abstract = {The prediction of physicochemical properties from molecular structures is a crucial task for artificial intelligence aided molecular design. A growing number of Graph Neural Networks (GNNs) have been proposed to address this challenge. These models improve their expressive power by incorporating auxiliary information in molecules while inevitably increase their computational complexity. In this work, we aim to design a GNN which is both powerful and efficient for molecule structures. To achieve such goal, we propose a molecular mechanics-driven approach by first representing each molecule as a two-layer multiplex graph, where one layer contains only local connections that mainly capture the covalent interactions and another layer contains global connections that can simulate non-covalent interactions. Then for each layer, a corresponding message passing module is proposed to balance the trade-off of expression power and computational complexity. Based on these two modules, we build Multiplex Molecular Graph Neural Network (MXMNet). When validated by the QM9 dataset for small molecules and PDBBind dataset for large protein-ligand complexes, MXMNet achieves superior results to the existing state-of-the-art models under restricted resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Quantitative Biology - Quantitative Methods},
  file = {/home/jsimonrichard/Zotero/storage/L47AW3T5/Zhang et al. - 2020 - Molecular Mechanics-Driven Graph Neural Network wi.pdf;/home/jsimonrichard/Zotero/storage/MIVC2GQ4/2011.html}
}

@article{zhangRecentAdvancesQuantum2020,
  title = {Recent Advances in Quantum Machine Learning},
  author = {Zhang, Yao and Ni, Qiang},
  year = {2020},
  journal = {Quantum Engineering},
  volume = {2},
  number = {1},
  pages = {e34},
  issn = {2577-0470},
  doi = {10.1002/que2.34},
  urldate = {2024-05-04},
  abstract = {Machine learning is a branch of artificial intelligence, and it has been widely used in many science and engineering areas, such as data mining, natural language processing, computer vision, biological analysis, and so on. Quantum computer is considered as one of the most promising technologies of human beings in the near future. With the development of machine learning and quantum computing, researchers consider to combine these two aspects to gain more benefits. As a result, a novel interdisciplinary subject has emerged---quantum machine learning. This article reviews the state-of-the-art research of algorithms of quantum machine learning and shows a path of the research from the basic quantum information to quantum machine learning algorithms from the perspective of people in the field of computer science.},
  copyright = {{\copyright} 2020 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {machine learning,quantum computing,quantum machine learning,quantum machine learning algorithms},
  file = {/home/jsimonrichard/Zotero/storage/ASMHAEFQ/Zhang and Ni - 2020 - Recent advances in quantum machine learning.pdf;/home/jsimonrichard/Zotero/storage/QEDC5QJ9/que2.html}
}

@misc{zhengQuantumGraphConvolutional2021,
  title = {Quantum {{Graph Convolutional Neural Networks}}},
  author = {Zheng, Jin and Gao, Qing and Lv, Yanxuan},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03257},
  eprint = {2107.03257},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.03257},
  urldate = {2024-01-23},
  abstract = {At present, there are a large number of quantum neural network models to deal with Euclidean spatial data, while little research have been conducted on non-Euclidean spatial data. In this paper, we propose a novel quantum graph convolutional neural network (QGCN) model based on quantum parametric circuits and utilize the computing power of quantum systems to accomplish graph classification tasks in traditional machine learning. The proposed QGCN model has a similar architecture as the classical graph convolutional neural networks, which can illustrate the topology of the graph type data and efficiently learn the hidden layer representation of node features as well. Numerical simulation results on a graph dataset demonstrate that the proposed model can be effectively trained and has good performance in graph level classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/jsimonrichard/Zotero/storage/7THW54U7/Zheng et al. - 2021 - Quantum Graph Convolutional Neural Networks.pdf;/home/jsimonrichard/Zotero/storage/XWMFFTDN/2107.html}
}

@article{zhouHybridQuantumClassical2023,
  title = {Hybrid Quantum--Classical Generative Adversarial Networks for Image Generation via Learning Discrete Distribution},
  author = {Zhou, Nan-Run and Zhang, Tian-Feng and Xie, Xin-Wen and Wu, Jun-Yun},
  year = {2023},
  month = jan,
  journal = {Signal Processing: Image Communication},
  volume = {110},
  pages = {116891},
  issn = {0923-5965},
  doi = {10.1016/j.image.2022.116891},
  urldate = {2023-10-07},
  abstract = {It has been reported that quantum generative adversarial networks have a potential exponential advantage over classical generative adversarial networks. However, quantum machine learning is difficult to find real applications in the near future due to the limitation of quantum devices. The structure of quantum generator is optimized to reduce the required parameters and make use of quantum devices to a greater extent. And an image generation scheme is designed based on quantum generative adversarial networks. Two structures of quantum generative adversarial networks are simulated on Bars and Stripes dataset, and the results corroborate that the quantum generator with reduced parameters has no visible performance loss. The original complex multimodal distribution of an image can be converted into a simple unimodal distribution by the remapping method. The MNIST images and the Fashion-MNIST images are successfully generated by the optimized quantum generator with the remapping method, which verified the feasibility of the proposed image generation scheme.},
  keywords = {Hybrid quantum-classical algorithms,Image generation,Quantum computation,Quantum generative adversarial network,Quantum machine learning},
  file = {/home/jsimonrichard/Zotero/storage/XPC2HPKA/Zhou et al. - 2023 - Hybrid quantum–classical generative adversarial ne.pdf;/home/jsimonrichard/Zotero/storage/LAQ3FNLP/S0923596522001709.html}
}
